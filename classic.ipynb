{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 01:23:49,253 - __main__ - INFO - Token 432294 idle for 0:49:34.253164. Will process on next tick.\n",
      "2024-10-09 01:23:50,259 - __main__ - INFO - Token 432294 idle for 0:49:35.259287. Will process on next tick.\n",
      "2024-10-09 01:23:51,265 - __main__ - INFO - Token 432294 idle for 0:49:36.265050. Will process on next tick.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pyotp\n",
    "from NorenRestApiPy.NorenApi import NorenApi\n",
    "import numba_indicators\n",
    "import nest_asyncio\n",
    "import weakref\n",
    "import numpy as np\n",
    "\n",
    "# Global variable to store the DataProcessor instance\n",
    "global_data_processor = None\n",
    "global_candle_end_finder = None\n",
    "global_tick_collector = None\n",
    "\n",
    "class TickCollector:\n",
    "    def __init__(self, credentials_file=\"usercred.xlsx\"):\n",
    "        self.processing_lock = asyncio.Lock()\n",
    "        self.api = None\n",
    "        self.feed_opened = False\n",
    "        self.ring_buffers = {}\n",
    "        self.resampled_buffers = {}\n",
    "        self.resampling_enabled = {}\n",
    "        self.last_tick_time = {}\n",
    "        self.active_subscriptions = set()\n",
    "        self.RING_BUFFER_SIZE = 1000\n",
    "        self.RING_BUFFER_RESAMPLE_SIZE = 1000\n",
    "        self.VALID_TIMEFRAMES = ['5s', '15s']\n",
    "        \n",
    "        self.logger = self._setup_logger()\n",
    "        self._initialize_api(credentials_file)\n",
    "\n",
    "    def _setup_logger(self):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        return logger\n",
    "\n",
    "    def _initialize_api(self, credentials_file):\n",
    "        self.api = NorenApi(\n",
    "            host=\"https://api.shoonya.com/NorenWClientTP/\",\n",
    "            websocket=\"wss://api.shoonya.com/NorenWSTP/\"\n",
    "        )\n",
    "        credentials = pd.read_excel(credentials_file)\n",
    "        user = credentials.iloc[0, 0]\n",
    "        password = credentials.iloc[0, 1]\n",
    "        vendor_code = credentials.iloc[0, 2]\n",
    "        app_key = credentials.iloc[0, 3]\n",
    "        imei = credentials.iloc[0, 4]\n",
    "        qr_code = credentials.iloc[0, 5]\n",
    "        factor2 = pyotp.TOTP(qr_code).now()\n",
    "\n",
    "        self.api.login_result = self.api.login(\n",
    "            userid=user,\n",
    "            password=password,\n",
    "            twoFA=factor2,\n",
    "            vendor_code=vendor_code,\n",
    "            api_secret=app_key,\n",
    "            imei=imei\n",
    "        )\n",
    "\n",
    "    def create_ring_buffers(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token not in self.ring_buffers:\n",
    "                self.ring_buffers[token] = deque(maxlen=self.RING_BUFFER_SIZE)\n",
    "                self.last_tick_time[token] = None\n",
    "                self.logger.info(f\"Created ring buffer for token: {token}\")\n",
    "\n",
    "    def create_resampled_buffers(self, tokens, timeframes):\n",
    "        for token in tokens:\n",
    "            if token not in self.resampled_buffers:\n",
    "                self.resampled_buffers[token] = {}\n",
    "                self.resampling_enabled[token] = {}\n",
    "                self.logger.info(f\"Created resampled buffers entry for token: {token}\")\n",
    "            \n",
    "            for timeframe in timeframes:\n",
    "                if timeframe not in self.resampled_buffers[token]:\n",
    "                    self.resampled_buffers[token][timeframe] = deque(maxlen=self.RING_BUFFER_RESAMPLE_SIZE)\n",
    "                    self.resampling_enabled[token][timeframe] = False\n",
    "                    self.logger.info(f\"Created resampled buffer for token {token} and timeframe: {timeframe}\")\n",
    "\n",
    "    async def set_resampling(self, token, timeframe, enable):\n",
    "        if token in self.resampling_enabled and timeframe in self.resampling_enabled[token]:\n",
    "            self.resampling_enabled[token][timeframe] = enable\n",
    "            self.logger.info(f\"Resampling {'enabled' if enable else 'disabled'} for token {token} and timeframe {timeframe}\")\n",
    "        else:\n",
    "            self.logger.warning(f\"Token {token} or timeframe {timeframe} not found in resampling_enabled\")\n",
    "\n",
    "    def event_handler_feed_update(self, tick_data):\n",
    "        try:\n",
    "            if 'lp' in tick_data and 'tk' in tick_data:\n",
    "                timest = datetime.fromtimestamp(int(tick_data['ft'])).isoformat()\n",
    "                token = tick_data['tk']\n",
    "                if token in self.ring_buffers:\n",
    "                    new_tick = {'tt': timest, 'ltp': float(tick_data['lp'])}\n",
    "                    self.ring_buffers[token].append(new_tick)\n",
    "                    self.last_tick_time[token] = datetime.fromisoformat(timest)\n",
    "                else:\n",
    "                    self.logger.warning(f\"Token {token} not found in ring buffers. Ignoring tick.\")\n",
    "        except (KeyError, ValueError) as e:\n",
    "            self.logger.error(f\"Error processing tick data: {e}\")\n",
    "\n",
    "    async def connect_and_subscribe(self):\n",
    "        retry_delay = 1\n",
    "        max_retry_delay = 32\n",
    "        max_retries = 10\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                self.api.start_websocket(\n",
    "                    order_update_callback=self.event_handler_order_update,\n",
    "                    subscribe_callback=self.event_handler_feed_update,\n",
    "                    socket_open_callback=self.open_callback,\n",
    "                    socket_close_callback=self.close_callback\n",
    "                )\n",
    "                await self.wait_for_feed_open(timeout=30)\n",
    "                self.logger.info(\"WebSocket connected successfully.\")\n",
    "                \n",
    "                #await self.manage_subscriptions('add', 'MCX|432294')\n",
    "                await self.manage_subscriptions('add', 'NSE|26009')\n",
    "                await self.manage_subscriptions('add', 'NSE|26000')\n",
    "                \n",
    "                retry_delay = 1\n",
    "                retries = 0\n",
    "                await self.monitor_connection()\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"WebSocket connection error: {e}\")\n",
    "                retries += 1\n",
    "                self.logger.info(f\"Reconnecting in {retry_delay} seconds... (Attempt {retries}/{max_retries})\")\n",
    "                await asyncio.sleep(retry_delay)\n",
    "                retry_delay = min(retry_delay * 2, max_retry_delay)\n",
    "\n",
    "        if retries >= max_retries:\n",
    "            self.logger.error(\"Max retries reached. Exiting.\")\n",
    "            raise Exception(\"Max retries reached\")\n",
    "\n",
    "    async def manage_subscriptions(self, command, subscription):\n",
    "        token = subscription.split('|')[1]\n",
    "\n",
    "        if command == 'add':\n",
    "            if subscription not in self.active_subscriptions:\n",
    "                self.api.subscribe([subscription])\n",
    "                self.active_subscriptions.add(subscription)\n",
    "                self.create_ring_buffers([token])\n",
    "                self.create_resampled_buffers([token], self.VALID_TIMEFRAMES)\n",
    "                self.logger.info(f\"Subscribed to {subscription}\")\n",
    "                for timeframe in self.VALID_TIMEFRAMES:\n",
    "                    await self.set_resampling(token, timeframe, True)\n",
    "                self.logger.info(f\"Resampling enabled for token {token} for all valid timeframes.\")\n",
    "            else:\n",
    "                self.logger.warning(f\"Already subscribed to {subscription}\")\n",
    "        elif command == 'remove':\n",
    "            if subscription in self.active_subscriptions:\n",
    "                self.api.unsubscribe([subscription])\n",
    "                self.active_subscriptions.remove(subscription)\n",
    "                self.logger.info(f\"Unsubscribed from {subscription}\")\n",
    "            else:\n",
    "                self.logger.warning(f\"Not subscribed to {subscription}\")\n",
    "\n",
    "    async def wait_for_feed_open(self, timeout):\n",
    "        start_time = asyncio.get_event_loop().time()\n",
    "        while not self.feed_opened:\n",
    "            if asyncio.get_event_loop().time() - start_time > timeout:\n",
    "                raise TimeoutError(\"Timed out waiting for feed to open\")\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "    async def monitor_connection(self):\n",
    "        while True:\n",
    "            if not self.feed_opened:\n",
    "                self.logger.warning(\"Feed closed unexpectedly. Reconnecting...\")\n",
    "                raise Exception(\"Feed closed\")\n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "    def close_callback(self):\n",
    "        self.feed_opened = False\n",
    "        self.logger.warning(\"WebSocket connection closed.\")\n",
    "        self.logger.info(\"Attempting to reconnect...\")\n",
    "\n",
    "    def open_callback(self):\n",
    "        if not self.feed_opened:\n",
    "            self.feed_opened = True\n",
    "            self.logger.info('Feed Opened')\n",
    "        else:\n",
    "            self.logger.warning('Feed Opened callback called multiple times.')\n",
    "\n",
    "    def event_handler_order_update(self, data):\n",
    "        self.logger.info(f\"Order update: {data}\")\n",
    "\n",
    "    async def run(self):\n",
    "        await asyncio.gather(\n",
    "            self.connect_and_subscribe()            \n",
    "        )\n",
    "\n",
    "\n",
    "import numba_indicators\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, tick_collector):        \n",
    "        self.tick_collector = tick_collector\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.last_processed_time = {}\n",
    "        self.IDLE_THRESHOLD = timedelta(minutes=1)\n",
    "\n",
    "    async def run(self):\n",
    "        await asyncio.gather(\n",
    "            self.process_data()            \n",
    "        )\n",
    "\n",
    "    async def ohlc_resampling_for_token(self, token, timeframe):\n",
    "        try:\n",
    "            async with self.tick_collector.processing_lock:\n",
    "                current_time = datetime.now()\n",
    "                last_tick_time = self.tick_collector.last_tick_time.get(token)\n",
    "                \n",
    "                if last_tick_time is None:\n",
    "                    self.logger.info(f\"No last tick time available for {token}\")\n",
    "                    return\n",
    "                \n",
    "                time_since_last_tick = current_time - last_tick_time\n",
    "                \n",
    "                if time_since_last_tick > self.IDLE_THRESHOLD:\n",
    "                    self.logger.info(f\"Token {token} idle for {time_since_last_tick}. Will process on next tick.\")\n",
    "                    return\n",
    "                \n",
    "                ticks = self.tick_collector.ring_buffers.get(token, deque())\n",
    "                if not ticks:\n",
    "                    self.logger.info(f\"No ticks available for {token}\")\n",
    "                    return\n",
    "                \n",
    "                df = pd.DataFrame(list(ticks))\n",
    "\n",
    "            # Release the lock for CPU-intensive operations\n",
    "            df['tt'] = pd.to_datetime(df['tt'])\n",
    "            df.set_index('tt', inplace=True)\n",
    "            \n",
    "            # Resampling\n",
    "            resampled = df['ltp'].resample(timeframe).ohlc()\n",
    "            #resampled['volume'] = df['v'].resample(timeframe).sum()\n",
    "            \n",
    "            # Calculate indicators\n",
    "            high = resampled['high'].values\n",
    "            low = resampled['low'].values\n",
    "            close = resampled['close'].values\n",
    "            \n",
    "            # high_array = np.array(high)\n",
    "            # low_array = np.array(low)\n",
    "            # close_array = np.array(close)\n",
    "            \n",
    "            supertrend, supertrend_direction = numba_indicators.supertrend_numba(high, low, close)\n",
    "            jma, jma_direction = numba_indicators.jma_numba_direction(close)\n",
    "\n",
    "            resampled['supertrend'] = supertrend\n",
    "            resampled['supertrend_direction'] = supertrend_direction\n",
    "            resampled['jma'] = jma\n",
    "            resampled['jma_direction'] = jma_direction\n",
    "            \n",
    "            # Update resampled buffer\n",
    "            async with self.tick_collector.processing_lock:\n",
    "                resampled_buffer = self.tick_collector.resampled_buffers[token].get(timeframe, deque(maxlen=self.tick_collector.RING_BUFFER_RESAMPLE_SIZE))\n",
    "                \n",
    "                new_records = resampled.reset_index().to_dict('records')\n",
    "                for record in new_records:\n",
    "                    # Update existing records or append new ones\n",
    "                    existing_record = next((item for item in resampled_buffer if item['tt'] == record['tt']), None)\n",
    "                    if existing_record:\n",
    "                        existing_record.update(record)\n",
    "                    else:\n",
    "                        resampled_buffer.append(record)\n",
    "                \n",
    "                self.tick_collector.resampled_buffers[token][timeframe] = resampled_buffer\n",
    "                self.last_processed_time[(token, timeframe)] = resampled.index[-1].to_pydatetime()\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing token {token} at timeframe {timeframe}: {str(e)}\")\n",
    "\n",
    "    async def process_data(self):        \n",
    "        while True:\n",
    "            for token in self.tick_collector.ring_buffers.keys():\n",
    "                for timeframe in self.tick_collector.VALID_TIMEFRAMES:\n",
    "                    if self.tick_collector.resampling_enabled[token].get(timeframe, False):\n",
    "                        await self.ohlc_resampling_for_token(token, timeframe)\n",
    "            await asyncio.sleep(1)  # Adjust as needed\n",
    "\n",
    "    async def get_resampled_buffer_contents(self, token=None, timeframe=None):    \n",
    "        if token is None and timeframe is None:\n",
    "            return {t: {tf: list(b) for tf, b in buffers.items()} \n",
    "                    for t, buffers in self.tick_collector.resampled_buffers.items()}\n",
    "        elif token is not None and timeframe is None:\n",
    "            return {tf: list(b) for tf, b in self.tick_collector.resampled_buffers.get(token, {}).items()}\n",
    "        elif token is not None and timeframe is not None:\n",
    "            return list(self.tick_collector.resampled_buffers.get(token, {}).get(timeframe, deque()))\n",
    "        else:  # token is None and timeframe is not None\n",
    "            return {t: list(buffers.get(timeframe, deque())) \n",
    "                    for t, buffers in self.tick_collector.resampled_buffers.items()}\n",
    "\n",
    "class CandleEndFinder:\n",
    "    def __init__(self, data_processor: DataProcessor):\n",
    "        self.data_processor = data_processor\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.completed_candles_dfs = {}\n",
    "        self.last_processed_candle = {}\n",
    "        self.new_candle_queue = asyncio.Queue()  # New queue for publishing candles\n",
    "\n",
    "    async def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                await self.find_completed_candles()\n",
    "                await asyncio.sleep(.1)  # Adjust as needed speed of candle detection\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in CandleEndFinder run loop: {e}\")\n",
    "                await asyncio.sleep(5)  # Wait a bit longer before retrying after an error\n",
    "\n",
    "    async def find_completed_candles(self):\n",
    "        current_time = pd.Timestamp.now()\n",
    "        \n",
    "        for token, timeframes in self.data_processor.tick_collector.resampled_buffers.items():\n",
    "            for timeframe, resampled_data in timeframes.items():\n",
    "                async with self.data_processor.tick_collector.processing_lock:\n",
    "                    if not self.data_processor.tick_collector.resampling_enabled[token].get(timeframe, False):\n",
    "                        continue\n",
    "\n",
    "                    if not resampled_data:\n",
    "                        self.logger.debug(f\"No resampled data for token {token} and timeframe {timeframe}\")\n",
    "                        continue\n",
    "\n",
    "                    # Check if there's new data to process\n",
    "                    last_processed = self.last_processed_candle.get(token, {}).get(timeframe)\n",
    "                    if last_processed:\n",
    "                        last_data_time = pd.Timestamp(resampled_data[-1]['tt'])\n",
    "                        if last_data_time <= pd.Timestamp(last_processed):\n",
    "                            self.logger.debug(f\"No new data for {token} {timeframe} since last processing\")\n",
    "                            continue\n",
    "\n",
    "                    # Copy data we need to process\n",
    "                    data_to_process = list(resampled_data)\n",
    "\n",
    "                # Process data outside the lock\n",
    "                try:\n",
    "                    df = pd.DataFrame(data_to_process)\n",
    "                    df.set_index('tt', inplace=True)\n",
    "                    \n",
    "                    freq = pd.Timedelta(timeframe)\n",
    "                    time_bucket_start = current_time.floor(freq)\n",
    "                    if len(df) <= 1:\n",
    "                        self.logger.debug(f\"Not enough data for {token} {timeframe}\")\n",
    "                        continue\n",
    "                    \n",
    "                    completed_candles = df[df.index < time_bucket_start]\n",
    "\n",
    "                    if not completed_candles.empty:\n",
    "                        last_completed_candle = completed_candles.iloc[-1].to_dict()\n",
    "                        last_completed_candle['tt'] = completed_candles.index[-1].isoformat()\n",
    "\n",
    "                        # Reacquire lock to update completed_candles_dfs\n",
    "                        async with self.data_processor.tick_collector.processing_lock:\n",
    "                            if token not in self.completed_candles_dfs:\n",
    "                                self.completed_candles_dfs[token] = {}\n",
    "                            if timeframe not in self.completed_candles_dfs[token]:\n",
    "                                self.completed_candles_dfs[token][timeframe] = deque(maxlen=self.data_processor.tick_collector.RING_BUFFER_RESAMPLE_SIZE)\n",
    "                            \n",
    "                            if (token not in self.last_processed_candle or\n",
    "                                timeframe not in self.last_processed_candle[token] or\n",
    "                                self.last_processed_candle[token][timeframe] < last_completed_candle['tt']):\n",
    "                                \n",
    "                                self.completed_candles_dfs[token][timeframe].append(last_completed_candle)\n",
    "                                self.last_processed_candle.setdefault(token, {})[timeframe] = last_completed_candle['tt']\n",
    "                                \n",
    "                                self.logger.debug(f\"Added new completed candle for {token} {timeframe}: {last_completed_candle}\")\n",
    "\n",
    "                                # Publish the new candle to the queue\n",
    "                                await self.new_candle_queue.put((token, timeframe, last_completed_candle))  ### pub sub\n",
    "                   \n",
    "                    else:\n",
    "                        self.logger.debug(f\"No completed candles for token {token} and timeframe {timeframe}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing token {token} and timeframe {timeframe}: {str(e)}\")\n",
    "                    self.logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "\n",
    "    async def get_completed_candles(self, token=None, timeframe=None):\n",
    "        if token is None and timeframe is None:\n",
    "            return {t: {tf: list(d) for tf, d in timeframes.items()} \n",
    "                    for t, timeframes in self.completed_candles_dfs.items()}\n",
    "        elif token is not None and timeframe is None:\n",
    "            return {tf: list(d) for tf, d in self.completed_candles_dfs.get(token, {}).items()}\n",
    "        elif token is not None and timeframe is not None:\n",
    "            return list(self.completed_candles_dfs.get(token, {}).get(timeframe, deque()))\n",
    "        else:  # token is None and timeframe is not None\n",
    "            return {t: list(timeframes.get(timeframe, deque())) \n",
    "                    for t, timeframes in self.completed_candles_dfs.items() if timeframe in timeframes}\n",
    "        \n",
    "class TradingDecisionMaker:\n",
    "    def __init__(self, candle_end_finder: CandleEndFinder):\n",
    "        self.candle_end_finder = candle_end_finder\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    async def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                token, timeframe, candle = await self.candle_end_finder.new_candle_queue.get()\n",
    "                await self.make_trading_decision(token, timeframe, candle)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in TradingDecisionMaker: {e}\")\n",
    "                await asyncio.sleep(.1)\n",
    "\n",
    "    async def make_trading_decision(self, token, timeframe, candle):\n",
    "        # Implement your trading decision logic here\n",
    "        self.logger.info(f\"New candle for {token} at {timeframe}: {candle}\")\n",
    "        \n",
    "        # Example decision logic (replace with your own):\n",
    "        if candle['supertrend_direction'] == 1 and candle['jma_direction'] == 1:\n",
    "            self.logger.info(f\"Bullish signal for {token} at {timeframe}\")\n",
    "            # Implement buy logic here\n",
    "        elif candle['supertrend_direction'] == -1 and candle['jma_direction'] == -1:\n",
    "            self.logger.info(f\"Bearish signal for {token} at {timeframe}\")\n",
    "            # Implement sell logic here\n",
    "        else:\n",
    "            self.logger.info(f\"No clear signal for {token} at {timeframe}\")\n",
    "\n",
    "        # Add more complex decision logic based on other indicators or multiple timeframes\n",
    "\n",
    "async def main():\n",
    "    global global_data_processor\n",
    "    global global_candle_end_finder, global_tick_collector\n",
    "    collector = TickCollector()\n",
    "    processor = DataProcessor(collector)\n",
    "    candle_finder = CandleEndFinder(processor)\n",
    "    trading_decission = TradingDecisionMaker(candle_finder)\n",
    "    \n",
    "    global_tick_collector = collector\n",
    "    global_data_processor = processor\n",
    "    global_candle_end_finder = candle_finder\n",
    "    \n",
    "    await asyncio.gather(collector.run(), processor.run(), candle_finder.run(), trading_decission.run())\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.set_debug(True)\n",
    "if loop.is_running():\n",
    "    nest_asyncio.apply()\n",
    "asyncio.create_task(main())\n",
    "if not loop.is_running():\n",
    "    try:\n",
    "        loop.run_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Received exit signal. Cleaning up...\")\n",
    "    finally:\n",
    "        loop.close()\n",
    "        logger.info(\"Event loop closed. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_completed_candles(token=None, timeframe=None):\n",
    "    global global_candle_end_finder    \n",
    "    if global_candle_end_finder is None:\n",
    "        raise ValueError(\"global_candle_end_finder is not initialized\")    \n",
    "    try:\n",
    "        completed_candles = await global_candle_end_finder.get_completed_candles(token, timeframe)\n",
    "        if completed_candles:\n",
    "            return completed_candles[-1]  # Return the last value\n",
    "        return None  # Return None if the list is empty\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting completed candles: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "async def example_usage():    \n",
    "    last_candle = await get_completed_candles(token=\"432294\", timeframe=\"5s\")\n",
    "    print(\"Last 1min candle for MCX|432294:\", last_candle)\n",
    "\n",
    "    last_candle = await get_completed_candles(token=\"432294\", timeframe=\"15s\")\n",
    "    print(\"Last 5min candle for MCX|432294:\", last_candle)\n",
    "\n",
    "    last_candle = await get_completed_candles(token=\"26000\", timeframe=\"5s\")\n",
    "    print(\"Last 1min candle for MCX|432294:\", last_candle)\n",
    "\n",
    "    last_candle = await get_completed_candles(token=\"26000\", timeframe=\"15s\")\n",
    "    print(\"Last 5min candle for MCX|432294:\", last_candle)\n",
    "\n",
    "# To run the example:\n",
    "asyncio.run(example_usage())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get resampled buffer contents\n",
    "async def get_buffer_contents(token=None, timeframe=None):\n",
    "    global global_data_processor\n",
    "    if global_data_processor is None:\n",
    "        return \"DataProcessor not initialized yet. Please wait a moment and try again.\"\n",
    "    return await global_data_processor.get_resampled_buffer_contents(token, timeframe)\n",
    "\n",
    "# Get all resampled buffer contents\n",
    "all_contents = await get_buffer_contents()\n",
    "print(\"All contents:\")\n",
    "for token, timeframes in all_contents.items():\n",
    "    print(f\"Token {token}:\")\n",
    "    for tf, data in timeframes.items():\n",
    "        print(f\"  Timeframe {tf}: {len(data)} entries\")\n",
    "        if data:\n",
    "            print(f\"    Latest entry: {data[-1]}\")  # Show only the latest entry\n",
    "\n",
    "# Get contents for a specific token (e.g., '432294')\n",
    "token_contents = await get_buffer_contents(token='26009')\n",
    "print(\"\\nContents for token 26009:\")\n",
    "for tf, data in token_contents.items():\n",
    "    print(f\"  Timeframe {tf}: {len(data)} entries\")\n",
    "    if data:\n",
    "        print(f\"    Latest entry: {data[-1]}\")  # Show only the latest entry\n",
    "\n",
    "# Get contents for a specific token and timeframe\n",
    "specific_contents = await get_buffer_contents(token='26009', timeframe='1min')\n",
    "print(\"\\nContents for token 26009, 1min timeframe:\")\n",
    "if specific_contents:\n",
    "    print(f\"  Latest entry: {specific_contents[-1]}\")  # Show only the latest entry\n",
    "else:\n",
    "    print(\"  No entries found.\")\n",
    "\n",
    "# Get contents for a specific timeframe across all tokens\n",
    "timeframe_contents = await get_buffer_contents(timeframe='5min')\n",
    "print(\"\\nContents for 5min timeframe:\")\n",
    "for token, data in timeframe_contents.items():\n",
    "    print(f\"  Token {token}: {len(data)} entries\")\n",
    "    if data:\n",
    "        print(f\"    Latest entry: {data[-1]}\")  # Show only the latest entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await TickCollector.manage_subscriptions('remove', 'MCX|432294')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
