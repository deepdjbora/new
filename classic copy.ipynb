{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pyotp\n",
    "from NorenRestApiPy.NorenApi import NorenApi\n",
    "import numba_indicators\n",
    "from typing import Dict, Any\n",
    "import nest_asyncio\n",
    "# Global variable to store the DataProcessor instance\n",
    "global_data_processor = None\n",
    "global_candle_end_finder = None\n",
    "\n",
    "class TickCollector:\n",
    "    def __init__(self, credentials_file=\"usercred.xlsx\"):\n",
    "        self.processing_lock = asyncio.Lock()\n",
    "        self.api = None\n",
    "        self.feed_opened = False\n",
    "        self.ring_buffers = {}\n",
    "        self.resampled_buffers = {}\n",
    "        self.resampling_enabled = {}\n",
    "        self.last_tick_time = {}\n",
    "        self.active_subscriptions = set()\n",
    "        self.RING_BUFFER_SIZE = 1000\n",
    "        self.RING_BUFFER_RESAMPLE_SIZE = 1000\n",
    "        self.VALID_TIMEFRAMES = ['5s', '15s']\n",
    "        \n",
    "        self.logger = self._setup_logger()\n",
    "        self._initialize_api(credentials_file)\n",
    "\n",
    "    def _setup_logger(self):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        return logger\n",
    "\n",
    "    def _initialize_api(self, credentials_file):\n",
    "        self.api = NorenApi(\n",
    "            host=\"https://api.shoonya.com/NorenWClientTP/\",\n",
    "            websocket=\"wss://api.shoonya.com/NorenWSTP/\"\n",
    "        )\n",
    "        credentials = pd.read_excel(credentials_file)\n",
    "        user = credentials.iloc[0, 0]\n",
    "        password = credentials.iloc[0, 1]\n",
    "        vendor_code = credentials.iloc[0, 2]\n",
    "        app_key = credentials.iloc[0, 3]\n",
    "        imei = credentials.iloc[0, 4]\n",
    "        qr_code = credentials.iloc[0, 5]\n",
    "        factor2 = pyotp.TOTP(qr_code).now()\n",
    "\n",
    "        self.api.login_result = self.api.login(\n",
    "            userid=user,\n",
    "            password=password,\n",
    "            twoFA=factor2,\n",
    "            vendor_code=vendor_code,\n",
    "            api_secret=app_key,\n",
    "            imei=imei\n",
    "        )\n",
    "\n",
    "    def create_ring_buffers(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token not in self.ring_buffers:\n",
    "                self.ring_buffers[token] = deque(maxlen=self.RING_BUFFER_SIZE)\n",
    "                self.last_tick_time[token] = None\n",
    "                self.logger.info(f\"Created ring buffer for token: {token}\")\n",
    "\n",
    "    def create_resampled_buffers(self, tokens, timeframes):\n",
    "        for token in tokens:\n",
    "            if token not in self.resampled_buffers:\n",
    "                self.resampled_buffers[token] = {}\n",
    "                self.resampling_enabled[token] = {}\n",
    "                self.logger.info(f\"Created resampled buffers entry for token: {token}\")\n",
    "            \n",
    "            for timeframe in timeframes:\n",
    "                if timeframe not in self.resampled_buffers[token]:\n",
    "                    self.resampled_buffers[token][timeframe] = deque(maxlen=self.RING_BUFFER_RESAMPLE_SIZE)\n",
    "                    self.resampling_enabled[token][timeframe] = False\n",
    "                    self.logger.info(f\"Created resampled buffer for token {token} and timeframe: {timeframe}\")\n",
    "\n",
    "    async def set_resampling(self, token, timeframe, enable):\n",
    "        if token in self.resampling_enabled and timeframe in self.resampling_enabled[token]:\n",
    "            self.resampling_enabled[token][timeframe] = enable\n",
    "            self.logger.info(f\"Resampling {'enabled' if enable else 'disabled'} for token {token} and timeframe {timeframe}\")\n",
    "        else:\n",
    "            self.logger.warning(f\"Token {token} or timeframe {timeframe} not found in resampling_enabled\")\n",
    "\n",
    "\n",
    "    def event_handler_feed_update(self, tick_data):\n",
    "        try:\n",
    "            if 'lp' in tick_data and 'tk' in tick_data:\n",
    "                timest = datetime.fromtimestamp(int(tick_data['ft'])).isoformat()\n",
    "                token = tick_data['tk']\n",
    "                if token in self.ring_buffers:\n",
    "                    new_tick = {'tt': timest, 'ltp': float(tick_data['lp'])}\n",
    "                    self.ring_buffers[token].append(new_tick)\n",
    "                    self.last_tick_time[token] = datetime.fromisoformat(timest)\n",
    "                else:\n",
    "                    self.logger.warning(f\"Token {token} not found in ring buffers. Ignoring tick.\")\n",
    "        except (KeyError, ValueError) as e:\n",
    "            self.logger.error(f\"Error processing tick data: {e}\")\n",
    "\n",
    " \n",
    "    async def connect_and_subscribe(self):\n",
    "        retry_delay = 1\n",
    "        max_retry_delay = 32\n",
    "        max_retries = 10\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                self.api.start_websocket(\n",
    "                    order_update_callback=self.event_handler_order_update,\n",
    "                    subscribe_callback=self.event_handler_feed_update,\n",
    "                    socket_open_callback=self.open_callback,\n",
    "                    socket_close_callback=self.close_callback\n",
    "                )\n",
    "                await self.wait_for_feed_open(timeout=30)\n",
    "                self.logger.info(\"WebSocket connected successfully.\")\n",
    "                \n",
    "                await self.manage_subscriptions('add', 'MCX|432294')\n",
    "                # await self.manage_subscriptions('add', 'NSE|26009')\n",
    "                # await self.manage_subscriptions('add', 'NSE|26000')\n",
    "                \n",
    "                retry_delay = 1\n",
    "                retries = 0\n",
    "                await self.monitor_connection()\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"WebSocket connection error: {e}\")\n",
    "                retries += 1\n",
    "                self.logger.info(f\"Reconnecting in {retry_delay} seconds... (Attempt {retries}/{max_retries})\")\n",
    "                await asyncio.sleep(retry_delay)\n",
    "                retry_delay = min(retry_delay * 2, max_retry_delay)\n",
    "\n",
    "        if retries >= max_retries:\n",
    "            self.logger.error(\"Max retries reached. Exiting.\")\n",
    "            raise Exception(\"Max retries reached\")\n",
    "\n",
    "    async def manage_subscriptions(self, command, subscription):\n",
    "        token = subscription.split('|')[1]\n",
    "\n",
    "        if command == 'add':\n",
    "            if subscription not in self.active_subscriptions:\n",
    "                self.api.subscribe([subscription])\n",
    "                self.active_subscriptions.add(subscription)\n",
    "                self.create_ring_buffers([token])\n",
    "                self.create_resampled_buffers([token], self.VALID_TIMEFRAMES)\n",
    "                self.logger.info(f\"Subscribed to {subscription}\")\n",
    "                for timeframe in self.VALID_TIMEFRAMES:\n",
    "                    await self.set_resampling(token, timeframe, True)\n",
    "                self.logger.info(f\"Resampling enabled for token {token} for all valid timeframes.\")\n",
    "            else:\n",
    "                self.logger.warning(f\"Already subscribed to {subscription}\")\n",
    "        elif command == 'remove':\n",
    "            if subscription in self.active_subscriptions:\n",
    "                self.api.unsubscribe([subscription])\n",
    "                self.active_subscriptions.remove(subscription)\n",
    "                self.logger.info(f\"Unsubscribed from {subscription}\")\n",
    "            else:\n",
    "                self.logger.warning(f\"Not subscribed to {subscription}\")\n",
    "\n",
    "    async def wait_for_feed_open(self, timeout):\n",
    "        start_time = asyncio.get_event_loop().time()\n",
    "        while not self.feed_opened:\n",
    "            if asyncio.get_event_loop().time() - start_time > timeout:\n",
    "                raise TimeoutError(\"Timed out waiting for feed to open\")\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "    async def monitor_connection(self):\n",
    "        while True:\n",
    "            if not self.feed_opened:\n",
    "                self.logger.warning(\"Feed closed unexpectedly. Reconnecting...\")\n",
    "                raise Exception(\"Feed closed\")\n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "    def close_callback(self):\n",
    "        self.feed_opened = False\n",
    "        self.logger.warning(\"WebSocket connection closed.\")\n",
    "        self.logger.info(\"Attempting to reconnect...\")\n",
    "\n",
    "    def open_callback(self):\n",
    "        if not self.feed_opened:\n",
    "            self.feed_opened = True\n",
    "            self.logger.info('Feed Opened')\n",
    "        else:\n",
    "            self.logger.warning('Feed Opened callback called multiple times.')\n",
    "\n",
    "    def event_handler_order_update(self, data):\n",
    "        self.logger.info(f\"Order update: {data}\")\n",
    "\n",
    "    async def run(self):\n",
    "        await asyncio.gather(\n",
    "            self.connect_and_subscribe()            \n",
    "        )\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, tick_collector:TickCollector):\n",
    "        self.tick_collector = tick_collector\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.last_processed_time = {}\n",
    "        self.IDLE_THRESHOLD = timedelta(minutes=1)\n",
    "             \n",
    "    async def ohlc_resampling(self):\n",
    "        async with self.tick_collector.processing_lock:\n",
    "            current_time = datetime.now()\n",
    "            for token, ticks in self.tick_collector.ring_buffers.items():\n",
    "                last_tick_time = self.tick_collector.last_tick_time.get(token) \n",
    "                \n",
    "                if last_tick_time is None:\n",
    "                    self.logger.info(f\"No last tick time available for {token}\")\n",
    "                    continue\n",
    "                \n",
    "                time_since_last_tick = current_time - last_tick_time\n",
    "                \n",
    "                if time_since_last_tick > self.IDLE_THRESHOLD:\n",
    "                    self.logger.info(f\"Token {token} idle for {time_since_last_tick}. Will process on next tick.\")\n",
    "                    continue\n",
    "                \n",
    "                if not ticks:\n",
    "                    self.logger.info(f\"No ticks available for {token}\")\n",
    "                    continue\n",
    "                \n",
    "                df = pd.DataFrame(list(ticks))\n",
    "                df['tt'] = pd.to_datetime(df['tt'])\n",
    "                df.set_index('tt', inplace=True)\n",
    "                \n",
    "                for timeframe in self.tick_collector.VALID_TIMEFRAMES:\n",
    "                    if self.tick_collector.resampling_enabled[token].get(timeframe, False):\n",
    "                        last_processed = self.last_processed_time.get((token, timeframe))\n",
    "                        \n",
    "                        if last_processed is None or last_tick_time > last_processed:\n",
    "                            resampled = df['ltp'].resample(timeframe).ohlc().dropna()\n",
    "                            \n",
    "                            if not resampled.empty:\n",
    "                                # Calculate indicators using resampled OHLC data\n",
    "                                high = resampled['high'].values\n",
    "                                low = resampled['low'].values\n",
    "                                close = resampled['close'].values\n",
    "                                \n",
    "                                # Call your indicator functions (assumed to be imported)\n",
    "                                supertrend, supertrend_direction = numba_indicators.supertrend_numba(high, low, close)\n",
    "                                jma, jma_direction = numba_indicators.jma_numba_direction(close)\n",
    "                                \n",
    "                                # Combine the calculated indicators into the resampled dataframe\n",
    "                                resampled['supertrend'] = supertrend\n",
    "                                resampled['supertrend_direction'] = supertrend_direction\n",
    "                                resampled['jma_direction'] = jma_direction\n",
    "                                \n",
    "                                # Merge with existing data without filling gaps\n",
    "                                existing_data = self.tick_collector.resampled_buffers[token].get(timeframe, deque())\n",
    "                                existing_df = pd.DataFrame(list(existing_data)).set_index('tt') if existing_data else pd.DataFrame()\n",
    "                                \n",
    "                                if not existing_df.empty:                                   \n",
    "                                    existing_df = existing_df[existing_df.index < resampled.index[0]]                                \n",
    "                                \n",
    "                                # Concatenate existing data with new data\n",
    "                                combined_df = pd.concat([existing_df, resampled])\n",
    "                                \n",
    "                                # Convert back to records and update the buffer\n",
    "                                resampled_records = combined_df.reset_index().to_dict('records')\n",
    "                                self.tick_collector.resampled_buffers[token][timeframe] = deque(\n",
    "                                    resampled_records,\n",
    "                                    maxlen=self.tick_collector.RING_BUFFER_RESAMPLE_SIZE\n",
    "                                )                                \n",
    "                                \n",
    "                                self.last_processed_time[(token, timeframe)] = resampled.index[-1].to_pydatetime()\n",
    "                                \n",
    "                            else:\n",
    "                                self.logger.info(f\"No resampled data for {token} at {timeframe}\")\n",
    "                        else:\n",
    "                            self.logger.info(f\"No new data for {token} at {timeframe} since last processing.\")\n",
    "                            \n",
    "    async def process_data(self):        \n",
    "        while True:          \n",
    "              \n",
    "            await self.ohlc_resampling()\n",
    "            await asyncio.sleep(1)  # Adjust the sleep time as needed\n",
    "\n",
    "    async def get_resampled_buffer_contents(self, token=None, timeframe=None):\n",
    "    \n",
    "        if token is None and timeframe is None:\n",
    "            return {t: {tf: list(b) for tf, b in buffers.items()} \n",
    "                    for t, buffers in self.tick_collector.resampled_buffers.items()}\n",
    "        elif token is not None and timeframe is None:\n",
    "            return {tf: list(b) for tf, b in self.tick_collector.resampled_buffers.get(token, {}).items()}\n",
    "        elif token is not None and timeframe is not None:\n",
    "            return list(self.tick_collector.resampled_buffers.get(token, {}).get(timeframe, deque()))\n",
    "        else:  # token is None and timeframe is not None\n",
    "            return {t: list(buffers.get(timeframe, deque())) \n",
    "                    for t, buffers in self.tick_collector.resampled_buffers.items()}\n",
    "\n",
    "    async def run(self):\n",
    "        await asyncio.gather(\n",
    "            self.process_data()            \n",
    "        )\n",
    "\n",
    "class CandleEndFinder:\n",
    "    def __init__(self, data_processor: DataProcessor):\n",
    "        self.data_processor = data_processor\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.completed_candles_dfs = {}\n",
    "        self.last_processed_candle = {}\n",
    "        #self.indicator_monitor = IndicatorMonitor(self) # new\n",
    "\n",
    "    async def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                await asyncio.gather(\n",
    "                    self.find_completed_candles()\n",
    "                ) ## new\n",
    "                await asyncio.sleep(1)  # Adjust as needed\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in CandleEndFinder run loop: {e}\")\n",
    "                await asyncio.sleep(5)  # Wait a bit longer before retrying after an error\n",
    "\n",
    "    async def find_completed_candles(self):\n",
    "        current_time = pd.Timestamp.now()\n",
    "        async with self.data_processor.tick_collector.processing_lock:\n",
    "            for token, timeframes in self.data_processor.tick_collector.resampled_buffers.items():\n",
    "                for timeframe, resampled_data in timeframes.items():\n",
    "                    if not self.data_processor.tick_collector.resampling_enabled[token].get(timeframe, False):\n",
    "                        continue\n",
    "\n",
    "                    if not resampled_data:\n",
    "                        self.logger.debug(f\"No resampled data for token {token} and timeframe {timeframe}\")\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        df = pd.DataFrame(list(resampled_data))\n",
    "                        self.logger.debug(f\"Resampled data for {token} {timeframe}: {df.head().to_dict()}\")\n",
    "                        \n",
    "                        # The 'tt' is already a Timestamp object, so we don't need to convert it\n",
    "                        df.set_index('tt', inplace=True)\n",
    "                        \n",
    "                        # Calculate the end of the current candle period\n",
    "                        freq = pd.Timedelta(timeframe)\n",
    "                        time_bucket_start = current_time.floor(freq)\n",
    "                        current_candle_end = time_bucket_start + freq\n",
    "                        # Find the last completed candle\n",
    "                        if len(df) <= 1:\n",
    "                            self.logger.debug(f\"Not enough data for {token} {timeframe}\")\n",
    "                            continue\n",
    "                        \n",
    "                        completed_candles = df[df.index < time_bucket_start]\n",
    "\n",
    "                        if not completed_candles.empty:\n",
    "                            last_completed_candle = completed_candles.iloc[-1].to_dict()\n",
    "                            # Add the index (timestamp) back to the dictionary\n",
    "                            last_completed_candle['tt'] = completed_candles.index[-1].isoformat()\n",
    "\n",
    "                            if token not in self.completed_candles_dfs:\n",
    "                                self.completed_candles_dfs[token] = {}\n",
    "                            if timeframe not in self.completed_candles_dfs[token]:\n",
    "                                self.completed_candles_dfs[token][timeframe] = deque(maxlen=self.data_processor.tick_collector.RING_BUFFER_RESAMPLE_SIZE)\n",
    "\n",
    "                            # Only append if it's a new candle\n",
    "                            if (token not in self.last_processed_candle or\n",
    "                                timeframe not in self.last_processed_candle[token] or\n",
    "                                self.last_processed_candle[token][timeframe] < last_completed_candle['tt']):\n",
    "                                \n",
    "                                self.completed_candles_dfs[token][timeframe].append(last_completed_candle)\n",
    "                                self.last_processed_candle.setdefault(token, {})[timeframe] = last_completed_candle['tt']                                \n",
    "                        else:\n",
    "                            self.logger.debug(f\"No completed candles for token {token} and timeframe {timeframe}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Error processing token {token} and timeframe {timeframe}: {str(e)}\")\n",
    "                        self.logger.error(f\"Resampled data: {resampled_data}\")\n",
    "                        import traceback\n",
    "                        self.logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "\n",
    "    async def get_completed_candles(self, token=None, timeframe=None):\n",
    "        if token is None and timeframe is None:\n",
    "            return {t: {tf: list(d) for tf, d in timeframes.items()} \n",
    "                    for t, timeframes in self.completed_candles_dfs.items()}\n",
    "        elif token is not None and timeframe is None:\n",
    "            return {tf: list(d) for tf, d in self.completed_candles_dfs.get(token, {}).items()}\n",
    "        elif token is not None and timeframe is not None:\n",
    "            return list(self.completed_candles_dfs.get(token, {}).get(timeframe, deque()))\n",
    "        else:  # token is None and timeframe is not None\n",
    "            return {t: list(timeframes.get(timeframe, deque())) \n",
    "                    for t, timeframes in self.completed_candles_dfs.items() if timeframe in timeframes}\n",
    "        \n",
    "# class IndicatorMonitor:\n",
    "#     def __init__(self, candle_finder):\n",
    "#         self.candle_end_finder = candle_finder\n",
    "#         self.previous_indicators: Dict[str, Dict[str, Dict[str, Any]]] = {}\n",
    "#         self.trade_generator = TradeGenerator(self)\n",
    "\n",
    "#     async def monitor_indicators(self):\n",
    "#         while True:\n",
    "#             for token, timeframes in self.candle_end_finder.completed_candles_dfs.items():\n",
    "#                 for timeframe, candles in timeframes.items():\n",
    "#                     if candles:\n",
    "#                         latest_candle = candles[-1]\n",
    "#                         await self.check_indicator_changes(token, timeframe, latest_candle)\n",
    "#             await asyncio.sleep(0.01)  # Adjust based on your required frequency\n",
    "\n",
    "#     async def check_indicator_changes(self, token: str, timeframe: str, latest_candle: Dict[str, Any]):\n",
    "#         if token not in self.previous_indicators:\n",
    "#             self.previous_indicators[token] = {}\n",
    "#         if timeframe not in self.previous_indicators[token]:\n",
    "#             self.previous_indicators[token][timeframe] = {}\n",
    "\n",
    "#         jma_direction = latest_candle.get('jma_direction')\n",
    "#         supertrend_direction = latest_candle.get('supertrend_direction')\n",
    "        \n",
    "#         previous_jma = self.previous_indicators[token][timeframe].get('jma_direction')\n",
    "#         previous_supertrend = self.previous_indicators[token][timeframe].get('supertrend_direction')\n",
    "\n",
    "#         if jma_direction != previous_jma or supertrend_direction != previous_supertrend:\n",
    "#             await self.trade_generator.generate_trade(token, timeframe, jma_direction, supertrend_direction, previous_jma)\n",
    "\n",
    "#         self.previous_indicators[token][timeframe]['jma_direction'] = jma_direction\n",
    "#         self.previous_indicators[token][timeframe]['supertrend_direction'] = supertrend_direction\n",
    "\n",
    "# class TradeGenerator:\n",
    "#     def __init__(self, indicator_monitor):\n",
    "#         self.indicator_monitor = indicator_monitor\n",
    "#         self.trade_executor = TradeExecutor(self)\n",
    "#         self.current_position = {}  # To keep track of open positions\n",
    "\n",
    "#     async def generate_trade(self, token: str, timeframe: str, jma_direction: int, supertrend_direction: int, previous_jma: int):\n",
    "#         if token not in self.current_position:\n",
    "#             self.current_position[token] = None\n",
    "\n",
    "#         # Exit logic: JMA direction change regardless of Supertrend\n",
    "#         if self.current_position[token] is not None and jma_direction != previous_jma:\n",
    "#             await self.trade_executor.execute_trade(token, 'EXIT', timeframe)\n",
    "#             self.current_position[token] = None\n",
    "\n",
    "#         # Entry logic: JMA and Supertrend aligned\n",
    "#         if self.current_position[token] is None:\n",
    "#             if jma_direction == 1 and supertrend_direction == 1:\n",
    "#                 await self.trade_executor.execute_trade(token, 'BUY', timeframe)\n",
    "#                 self.current_position[token] = 'LONG'\n",
    "#             elif jma_direction == -1 and supertrend_direction == -1:\n",
    "#                 await self.trade_executor.execute_trade(token, 'SELL', timeframe)\n",
    "#                 self.current_position[token] = 'SHORT'\n",
    "\n",
    "# class TradeExecutor:\n",
    "#     def __init__(self, trade_generator):\n",
    "#         self.trade_generator = trade_generator\n",
    "\n",
    "#     async def execute_trade(self, token: str, action: str, timeframe: str):\n",
    "#         api = self.trade_generator.indicator_monitor.candle_end_finder.data_processor.tick_collector.api\n",
    "        \n",
    "#         # Get the latest price\n",
    "#         latest_candle = self.trade_generator.indicator_monitor.candle_end_finder.completed_candles_dfs[token][timeframe][-1]\n",
    "#         price = latest_candle['close']\n",
    "\n",
    "#         # Prepare the order\n",
    "#         if action == 'BUY':\n",
    "#             order_type = 'B'\n",
    "#         elif action == 'SELL':\n",
    "#             order_type = 'S'\n",
    "#         elif action == 'EXIT':\n",
    "#             order_type = 'S' if self.trade_generator.current_position[token] == 'LONG' else 'B'\n",
    "\n",
    "#         # Place the order\n",
    "#         try:\n",
    "#             order = api.place_order(\n",
    "#                 buy_or_sell=order_type,\n",
    "#                 product_type='I',\n",
    "#                 exchange='NSE',\n",
    "#                 tradingsymbol=token,\n",
    "#                 quantity=1,  # Adjust quantity as needed\n",
    "#                 discloseqty=0,\n",
    "#                 price_type='MKT',\n",
    "#                 price=0,\n",
    "#                 trigger_price=None,\n",
    "#                 retention='DAY',\n",
    "#                 remarks=f'HFT_{timeframe}_{action}'\n",
    "#             )\n",
    "#             print(f\"Order placed: {order}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error placing order: {e}\")\n",
    "\n",
    "async def main():\n",
    "    global global_data_processor\n",
    "    global global_candle_end_finder\n",
    "    collector = TickCollector()\n",
    "    processor = DataProcessor(collector)\n",
    "    candle_finder = CandleEndFinder(processor)\n",
    "    \n",
    "    global_data_processor = processor\n",
    "    global_candle_end_finder = candle_finder\n",
    "    \n",
    "    await asyncio.gather(collector.run(), processor.run(), candle_finder.run())\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.set_debug(True)\n",
    "if loop.is_running():\n",
    "    nest_asyncio.apply()\n",
    "asyncio.create_task(main())\n",
    "if not loop.is_running():\n",
    "    try:\n",
    "        loop.run_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Received exit signal. Cleaning up...\")\n",
    "    finally:\n",
    "        loop.close()\n",
    "        logger.info(\"Event loop closed. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def get_completed_candles(token=None, timeframe=None):\n",
    "    global global_candle_end_finder    \n",
    "    if global_candle_end_finder is None:\n",
    "        raise ValueError(\"global_candle_end_finder is not initialized\")    \n",
    "    try:\n",
    "        return await global_candle_end_finder.get_completed_candles(token, timeframe)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting completed candles: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "async def example_usage():    \n",
    "    specific_candles = await get_completed_candles(token=\"432294\", timeframe=\"5s\")\n",
    "    print(\"1min candles for MCX|432294:\", specific_candles)\n",
    "\n",
    "    specific_candles = await get_completed_candles(token=\"432294\", timeframe=\"15s\")\n",
    "    print(\"5min candles for MCX|432294:\", specific_candles)\n",
    "\n",
    "    # specific_candles = await get_completed_candles(token=\"432294\", timeframe=\"5s\")\n",
    "    # print(\"1min candles for MCX|432294:\", specific_candles)\n",
    "\n",
    "    # specific_candles = await get_completed_candles(token=\"432294\", timeframe=\"15s\")\n",
    "    # print(\"5min candles for MCX|432294:\", specific_candles)\n",
    "\n",
    "# To run the example:\n",
    "asyncio.run(example_usage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get resampled buffer contents\n",
    "async def get_buffer_contents(token=None, timeframe=None):\n",
    "    global global_data_processor\n",
    "    if global_data_processor is None:\n",
    "        return \"DataProcessor not initialized yet. Please wait a moment and try again.\"\n",
    "    return await global_data_processor.get_resampled_buffer_contents(token, timeframe)\n",
    "# Get all resampled buffer contents\n",
    "all_contents = await get_buffer_contents()\n",
    "print(\"All contents:\")\n",
    "for token, timeframes in all_contents.items():\n",
    "    print(f\"Token {token}:\")\n",
    "    for tf, data in timeframes.items():\n",
    "        print(f\"  Timeframe {tf}: {len(data)} entries\")\n",
    "        if data:\n",
    "            print(f\"    complete  entry: {data}\")\n",
    "\n",
    "# Get contents for a specific token (e.g., '432294')\n",
    "token_contents = await get_buffer_contents(token='432294')\n",
    "print(\"\\nContents for token 432294:\")\n",
    "for tf, data in token_contents.items():\n",
    "    print(f\"  Timeframe {tf}: {len(data)} entries\")\n",
    "    if data:\n",
    "        print(f\"    Latest entry: {data[-1]}\")\n",
    "\n",
    "# Get contents for a specific token and timeframe\n",
    "specific_contents = await get_buffer_contents(token='432294', timeframe='1min')\n",
    "print(\"\\nContents for token 432294, 1min timeframe:\")\n",
    "print(f\"  {len(specific_contents)} entries\")\n",
    "if specific_contents:\n",
    "    print(f\"    Latest entry: {specific_contents[-1]}\")\n",
    "\n",
    "# Get contents for a specific timeframe across all tokens\n",
    "timeframe_contents = await get_buffer_contents(timeframe='5min')\n",
    "print(\"\\nContents for 5min timeframe:\")\n",
    "for token, data in timeframe_contents.items():\n",
    "    print(f\"  Token {token}: {len(data)} entries\")\n",
    "    if data:\n",
    "        print(f\"    Latest entry: {data[-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
