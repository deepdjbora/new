{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pyotp\n",
    "from NorenRestApiPy.NorenApi import NorenApi\n",
    "import numba_indicators\n",
    "import nest_asyncio\n",
    "import weakref\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Global variable to store the DataProcessor instance\n",
    "global_data_processor = None\n",
    "global_candle_end_finder = None\n",
    "global_tick_collector = None\n",
    "global_decision_maker = None\n",
    "\n",
    "class TickCollector:\n",
    "    def __init__(self, credentials_file=\"usercred.xlsx\"):\n",
    "        self.processing_lock = asyncio.Lock()\n",
    "        self.api = None\n",
    "        self.feed_opened = False\n",
    "        self.ring_buffers = {}\n",
    "        self.resampled_buffers = {}\n",
    "        self.resampling_enabled = {}\n",
    "        self.last_tick_time = {}\n",
    "        self.active_subscriptions = set()\n",
    "        self.RING_BUFFER_SIZE = 1000\n",
    "        self.RING_BUFFER_RESAMPLE_SIZE = 1000\n",
    "        self.VALID_TIMEFRAMES = ['5s','15s']\n",
    "        \n",
    "        self.logger = self._setup_logger()\n",
    "        self._initialize_api(credentials_file)\n",
    "\n",
    "    def _setup_logger(self):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        return logger\n",
    "\n",
    "    def _initialize_api(self, credentials_file):\n",
    "        self.api = NorenApi(\n",
    "            host=\"https://api.shoonya.com/NorenWClientTP/\",\n",
    "            websocket=\"wss://api.shoonya.com/NorenWSTP/\"\n",
    "        )\n",
    "        credentials = pd.read_excel(credentials_file)\n",
    "        user = credentials.iloc[0, 0]\n",
    "        password = credentials.iloc[0, 1]\n",
    "        vendor_code = credentials.iloc[0, 2]\n",
    "        app_key = credentials.iloc[0, 3]\n",
    "        imei = credentials.iloc[0, 4]\n",
    "        qr_code = credentials.iloc[0, 5]\n",
    "        factor2 = pyotp.TOTP(qr_code).now()\n",
    "\n",
    "        self.api.login_result = self.api.login(\n",
    "            userid=user,\n",
    "            password=password,\n",
    "            twoFA=factor2,\n",
    "            vendor_code=vendor_code,\n",
    "            api_secret=app_key,\n",
    "            imei=imei\n",
    "        )\n",
    "\n",
    "    def create_ring_buffers(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token not in self.ring_buffers:\n",
    "                self.ring_buffers[token] = deque(maxlen=self.RING_BUFFER_SIZE)\n",
    "                self.last_tick_time[token] = None\n",
    "                self.logger.info(f\"Created ring buffer for token: {token}\")\n",
    "\n",
    "    def create_resampled_buffers(self, tokens, timeframes):\n",
    "        for token in tokens:\n",
    "            if token not in self.resampled_buffers:\n",
    "                self.resampled_buffers[token] = {}\n",
    "                self.resampling_enabled[token] = {}\n",
    "                self.logger.info(f\"Created resampled buffers entry for token: {token}\")\n",
    "            \n",
    "            for timeframe in timeframes:\n",
    "                if timeframe not in self.resampled_buffers[token]:\n",
    "                    self.resampled_buffers[token][timeframe] = deque(maxlen=self.RING_BUFFER_RESAMPLE_SIZE)\n",
    "                    self.resampling_enabled[token][timeframe] = False\n",
    "                    self.logger.info(f\"Created resampled buffer for token {token} and timeframe: {timeframe}\")\n",
    "\n",
    "    async def set_resampling(self, token, timeframe, enable):\n",
    "        if token in self.resampling_enabled and timeframe in self.resampling_enabled[token]:\n",
    "            self.resampling_enabled[token][timeframe] = enable\n",
    "            self.logger.info(f\"Resampling {'enabled' if enable else 'disabled'} for token {token} and timeframe {timeframe}\")\n",
    "        else:\n",
    "            self.logger.warning(f\"Token {token} or timeframe {timeframe} not found in resampling_enabled\")\n",
    "\n",
    "    def event_handler_feed_update(self, tick_data):\n",
    "        try:\n",
    "            if 'lp' in tick_data and 'tk' in tick_data:\n",
    "                timest = datetime.fromtimestamp(int(tick_data['ft'])).isoformat()\n",
    "                token = tick_data['tk']\n",
    "                if token in self.ring_buffers:\n",
    "                    new_tick = {'tt': timest, 'ltp': float(tick_data['lp'])}\n",
    "                    self.ring_buffers[token].append(new_tick)\n",
    "                    self.last_tick_time[token] = datetime.fromisoformat(timest)\n",
    "                else:\n",
    "                    self.logger.warning(f\"Token {token} not found in ring buffers. Ignoring tick.\")\n",
    "        except (KeyError, ValueError) as e:\n",
    "            self.logger.error(f\"Error processing tick data: {e}\")\n",
    "\n",
    "    async def connect_and_subscribe(self):\n",
    "        retry_delay = 1\n",
    "        max_retry_delay = 32\n",
    "        max_retries = 10\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                self.api.start_websocket(\n",
    "                    order_update_callback=self.event_handler_order_update,\n",
    "                    subscribe_callback=self.event_handler_feed_update,\n",
    "                    socket_open_callback=self.open_callback,\n",
    "                    socket_close_callback=self.close_callback\n",
    "                )\n",
    "                await self.wait_for_feed_open(timeout=30)\n",
    "                self.logger.info(\"WebSocket connected successfully.\")\n",
    "                \n",
    "                await self.manage_subscriptions('add', 'MCX|432294')\n",
    "                #await self.manage_subscriptions('add', 'NSE|26009')\n",
    "                # await self.manage_subscriptions('add', 'NSE|26000')\n",
    "                \n",
    "                retry_delay = 1\n",
    "                retries = 0\n",
    "                await self.monitor_connection()\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"WebSocket connection error: {e}\")\n",
    "                retries += 1\n",
    "                self.logger.info(f\"Reconnecting in {retry_delay} seconds... (Attempt {retries}/{max_retries})\")\n",
    "                await asyncio.sleep(retry_delay)\n",
    "                retry_delay = min(retry_delay * 2, max_retry_delay)\n",
    "\n",
    "        if retries >= max_retries:\n",
    "            self.logger.error(\"Max retries reached. Exiting.\")\n",
    "            raise Exception(\"Max retries reached\")\n",
    "\n",
    "    async def manage_subscriptions(self, command, subscription):\n",
    "        token = subscription.split('|')[1]\n",
    "\n",
    "        if command == 'add':\n",
    "            if subscription not in self.active_subscriptions:\n",
    "                self.api.subscribe([subscription])\n",
    "                self.active_subscriptions.add(subscription)\n",
    "                self.create_ring_buffers([token])\n",
    "                self.create_resampled_buffers([token], self.VALID_TIMEFRAMES)\n",
    "                self.logger.info(f\"Subscribed to {subscription}\")\n",
    "                for timeframe in self.VALID_TIMEFRAMES:\n",
    "                    await self.set_resampling(token, timeframe, True)\n",
    "                self.logger.info(f\"Resampling enabled for token {token} for all valid timeframes.\")\n",
    "            else:\n",
    "                self.logger.warning(f\"Already subscribed to {subscription}\")\n",
    "        elif command == 'remove':\n",
    "            if subscription in self.active_subscriptions:\n",
    "                self.api.unsubscribe([subscription])\n",
    "                self.active_subscriptions.remove(subscription)\n",
    "                self.logger.info(f\"Unsubscribed from {subscription}\")\n",
    "            else:\n",
    "                self.logger.warning(f\"Not subscribed to {subscription}\")\n",
    "\n",
    "    async def wait_for_feed_open(self, timeout):\n",
    "        start_time = asyncio.get_event_loop().time()\n",
    "        while not self.feed_opened:\n",
    "            if asyncio.get_event_loop().time() - start_time > timeout:\n",
    "                raise TimeoutError(\"Timed out waiting for feed to open\")\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "    async def monitor_connection(self):\n",
    "        while True:\n",
    "            if not self.feed_opened:\n",
    "                self.logger.warning(\"Feed closed unexpectedly. Reconnecting...\")\n",
    "                raise Exception(\"Feed closed\")\n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "    def close_callback(self):\n",
    "        self.feed_opened = False\n",
    "        self.logger.warning(\"WebSocket connection closed.\")\n",
    "        self.logger.info(\"Attempting to reconnect...\")\n",
    "\n",
    "    def open_callback(self):\n",
    "        if not self.feed_opened:\n",
    "            self.feed_opened = True\n",
    "            self.logger.info('Feed Opened')\n",
    "        else:\n",
    "            self.logger.warning('Feed Opened callback called multiple times.')\n",
    "\n",
    "    def event_handler_order_update(self, data):\n",
    "        self.logger.info(f\"Order update: {data}\")\n",
    "\n",
    "    async def run(self):\n",
    "        await asyncio.gather(\n",
    "            self.connect_and_subscribe()            \n",
    "        )\n",
    "        \n",
    "class DataProcessor:\n",
    "    def __init__(self, tick_collector):\n",
    "        self.tick_collector = tick_collector\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.last_processed_time = {}\n",
    "        self.IDLE_THRESHOLD = timedelta(minutes=1)        \n",
    "\n",
    "    async def run(self):\n",
    "        while True:\n",
    "            await asyncio.sleep(0.1)  # Reduced sleep time\n",
    "            await self.process_data()\n",
    "\n",
    "    async def process_data(self):\n",
    "        try:\n",
    "            tokens_to_process = []\n",
    "            async with self.tick_collector.processing_lock:\n",
    "                current_time = datetime.now()\n",
    "                for token in self.tick_collector.ring_buffers.keys():\n",
    "                    last_tick_time = self.tick_collector.last_tick_time.get(token)\n",
    "                    if last_tick_time is None:\n",
    "                        self.logger.info(f\"No last tick time available for {token}\")\n",
    "                        continue\n",
    "                    \n",
    "                    time_since_last_tick = current_time - last_tick_time\n",
    "                    if time_since_last_tick <= self.IDLE_THRESHOLD:\n",
    "                        tokens_to_process.append(token)\n",
    "\n",
    "            for token in tokens_to_process:\n",
    "                for timeframe in self.tick_collector.VALID_TIMEFRAMES:\n",
    "                    if self.tick_collector.resampling_enabled[token].get(timeframe, False):\n",
    "                        await self.process_token_timeframe(token, timeframe)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in process_data: {str(e)}\")\n",
    "            self.logger.exception(\"Detailed traceback:\")\n",
    "\n",
    "    async def process_token_timeframe(self, token, timeframe):\n",
    "        try:\n",
    "            ticks = None\n",
    "            async with self.tick_collector.processing_lock:\n",
    "                ticks = list(self.tick_collector.ring_buffers.get(token, deque()))\n",
    "            \n",
    "            if not ticks:\n",
    "                return\n",
    "\n",
    "            df_new = pd.DataFrame(ticks)\n",
    "            df_new[\"tt\"] = pd.to_datetime(df_new[\"tt\"])\n",
    "            df_new.set_index(\"tt\", inplace=True)\n",
    "\n",
    "            df_resampled = df_new['ltp'].resample(timeframe).ohlc()\n",
    "            df_resampled = df_resampled.dropna(subset=['open', 'high', 'low', 'close'])\n",
    "\n",
    "            # Indicator calculation\n",
    "            try:\n",
    "                supertrend, supertrend_direction = numba_indicators.supertrend_numba(\n",
    "                    df_resampled['high'].values,\n",
    "                    df_resampled['low'].values,\n",
    "                    df_resampled['close'].values\n",
    "                )\n",
    "                df_resampled['supertrend'] = supertrend\n",
    "                df_resampled['supertrend_direction'] = supertrend_direction\n",
    "\n",
    "                jma, jma_direction = numba_indicators.jma_numba_direction(\n",
    "                    df_resampled['close'].values\n",
    "                )\n",
    "                df_resampled['jma'] = jma\n",
    "                df_resampled['jma_direction'] = jma_direction\n",
    "\n",
    "                self.logger.debug(f\"Indicator calculation completed for {token} at {timeframe}. Data shape: {df_resampled.shape}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error calculating indicators for {token} at {timeframe}: {str(e)}\")\n",
    "                self.logger.exception(\"Detailed traceback:\")\n",
    "                return\n",
    "\n",
    "            new_records = df_resampled.reset_index().to_dict('records')\n",
    "            \n",
    "            async with self.tick_collector.processing_lock:\n",
    "                resampled_buffer = self.tick_collector.resampled_buffers[token].get(timeframe, deque(maxlen=self.tick_collector.RING_BUFFER_RESAMPLE_SIZE))\n",
    "                resampled_buffer.clear()\n",
    "                resampled_buffer.extend(new_records)\n",
    "                self.tick_collector.resampled_buffers[token][timeframe] = resampled_buffer\n",
    "                self.last_processed_time[(token, timeframe)] = df_resampled.index.max()\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing {token} at {timeframe}: {str(e)}\")\n",
    "            self.logger.exception(\"Detailed traceback:\")\n",
    "\n",
    "    async def get_resampled_buffer_contents(self, token=None, timeframe=None):    \n",
    "        if token is None and timeframe is None:\n",
    "            return {t: {tf: list(b) for tf, b in buffers.items()} \n",
    "                    for t, buffers in self.tick_collector.resampled_buffers.items()}\n",
    "        elif token is not None and timeframe is None:\n",
    "            return {tf: list(b) for tf, b in self.tick_collector.resampled_buffers.get(token, {}).items()}\n",
    "        elif token is not None and timeframe is not None:\n",
    "            return list(self.tick_collector.resampled_buffers.get(token, {}).get(timeframe, deque()))\n",
    "        else:  # token is None and timeframe is not None\n",
    "            return {t: list(buffers.get(timeframe, deque())) \n",
    "                    for t, buffers in self.tick_collector.resampled_buffers.items()}\n",
    "        \n",
    "class CandleEndFinder:\n",
    "    def __init__(self, data_processor: DataProcessor):\n",
    "        self.data_processor = data_processor\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.completed_candles_dfs = {}\n",
    "        self.last_processed_candle = {}\n",
    "        self.new_candle_queue = asyncio.Queue()  # New queue for publishing candles\n",
    "        self.IDLE_THRESHOLD = timedelta(minutes=1) \n",
    "\n",
    "    async def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                await self.find_completed_candles()\n",
    "                await asyncio.sleep(.5)  # Adjust as needed speed of candle detection\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in CandleEndFinder run loop: {e}\")\n",
    "                await asyncio.sleep(5)  # Wait a bit longer before retrying after an error\n",
    "\n",
    "    async def find_completed_candles(self):\n",
    "        current_time = pd.Timestamp.now()\n",
    "        active_tokens = False\n",
    "        \n",
    "        for token, timeframes in self.data_processor.tick_collector.resampled_buffers.items():\n",
    "            last_tick_time = self.data_processor.tick_collector.last_tick_time.get(token)\n",
    "            if last_tick_time is None or (current_time - last_tick_time) > self.IDLE_THRESHOLD:\n",
    "                self.logger.debug(f\"Token {token} idle. Skipping candle end finding.\")\n",
    "                continue\n",
    "\n",
    "            active_tokens = True\n",
    "\n",
    "            for timeframe, resampled_data in timeframes.items():\n",
    "                async with self.data_processor.tick_collector.processing_lock:\n",
    "                    if not self.data_processor.tick_collector.resampling_enabled[token].get(timeframe, False):\n",
    "                        continue\n",
    "\n",
    "                    if not resampled_data:\n",
    "                        self.logger.debug(f\"No resampled data for token {token} and timeframe {timeframe}\")\n",
    "                        continue\n",
    "\n",
    "                    # Check if there's new data to process\n",
    "                    last_processed = self.last_processed_candle.get(token, {}).get(timeframe)\n",
    "                    if last_processed:\n",
    "                        last_data_time = pd.Timestamp(resampled_data[-1]['tt'])\n",
    "                        if last_data_time <= pd.Timestamp(last_processed):\n",
    "                            self.logger.debug(f\"No new data for {token} {timeframe} since last processing\")\n",
    "                            continue\n",
    "\n",
    "                    # Copy data we need to process\n",
    "                    data_to_process = list(resampled_data)\n",
    "\n",
    "                # Process data outside the lock\n",
    "                try:\n",
    "                    df = pd.DataFrame(data_to_process)\n",
    "                    df.set_index('tt', inplace=True)\n",
    "                    \n",
    "                    freq = pd.Timedelta(timeframe)\n",
    "                    time_bucket_start = current_time.floor(freq)\n",
    "                    if len(df) <= 1:\n",
    "                        self.logger.debug(f\"Not enough data for {token} {timeframe}\")\n",
    "                        continue\n",
    "                    \n",
    "                    completed_candles = df[df.index < time_bucket_start]\n",
    "\n",
    "                    if not completed_candles.empty:\n",
    "                        last_completed_candle = completed_candles.iloc[-1].to_dict()\n",
    "                        last_completed_candle['tt'] = completed_candles.index[-1].isoformat()\n",
    "\n",
    "                        # Reacquire lock to update completed_candles_dfs\n",
    "                        async with self.data_processor.tick_collector.processing_lock:\n",
    "                            if token not in self.completed_candles_dfs:\n",
    "                                self.completed_candles_dfs[token] = {}\n",
    "                            if timeframe not in self.completed_candles_dfs[token]:\n",
    "                                self.completed_candles_dfs[token][timeframe] = deque(maxlen=self.data_processor.tick_collector.RING_BUFFER_RESAMPLE_SIZE)\n",
    "                            \n",
    "                            if (token not in self.last_processed_candle or\n",
    "                                timeframe not in self.last_processed_candle[token] or\n",
    "                                self.last_processed_candle[token][timeframe] < last_completed_candle['tt']):\n",
    "                                \n",
    "                                self.completed_candles_dfs[token][timeframe].append(last_completed_candle)\n",
    "                                self.last_processed_candle.setdefault(token, {})[timeframe] = last_completed_candle['tt']\n",
    "                                \n",
    "                                self.logger.debug(f\"Added new completed candle for {token} {timeframe}: {last_completed_candle}\")\n",
    "\n",
    "                                # Publish the new candle to the queue\n",
    "                                await self.new_candle_queue.put((token, timeframe, last_completed_candle))  ### pub sub\n",
    "                   \n",
    "                    else:\n",
    "                        self.logger.debug(f\"No completed candles for token {token} and timeframe {timeframe}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing token {token} and timeframe {timeframe}: {str(e)}\")\n",
    "                    self.logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "\n",
    "        if not active_tokens:\n",
    "            await asyncio.sleep(5)    \n",
    "\n",
    "    async def get_completed_candles(self, token=None, timeframe=None):\n",
    "        if token is None and timeframe is None:\n",
    "            return {t: {tf: list(d) for tf, d in timeframes.items()} \n",
    "                    for t, timeframes in self.completed_candles_dfs.items()}\n",
    "        elif token is not None and timeframe is None:\n",
    "            return {tf: list(d) for tf, d in self.completed_candles_dfs.get(token, {}).items()}\n",
    "        elif token is not None and timeframe is not None:\n",
    "            return list(self.completed_candles_dfs.get(token, {}).get(timeframe, deque()))\n",
    "        else:  # token is None and timeframe is not None\n",
    "            return {t: list(timeframes.get(timeframe, deque())) \n",
    "                    for t, timeframes in self.completed_candles_dfs.items() if timeframe in timeframes}\n",
    "        \n",
    "class TradingDecisionMaker:\n",
    "    def __init__(self, candle_end_finder: CandleEndFinder):\n",
    "        self.candle_end_finder = candle_end_finder\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.log_directory = \"trading_logs\"\n",
    "        os.makedirs(self.log_directory, exist_ok=True)\n",
    "        \n",
    "        # Store last 10 candles for each token and timeframe\n",
    "        self.candle_history = {}\n",
    "        \n",
    "        # Control which tokens to trade\n",
    "        self.active_tokens  = set()\n",
    "        \n",
    "        # Control which indicators to use for trading decisions\n",
    "        self.active_indicators = {\n",
    "            'supertrend': True,\n",
    "            'jma': True,\n",
    "            'candle_average': False\n",
    "        }\n",
    "        \n",
    "        # Control automated trading\n",
    "        self.automated_trading_enabled = False\n",
    "\n",
    "    async def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                token, timeframe, candle = await self.candle_end_finder.new_candle_queue.get()\n",
    "                await self.process_candle(token, timeframe, candle)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in TradingDecisionMaker: {e}\")\n",
    "                await asyncio.sleep(0.1)\n",
    "\n",
    "    async def process_candle(self, token, timeframe, candle):\n",
    "        # Update candle history\n",
    "        self.update_candle_history(token, timeframe, candle)\n",
    "        \n",
    "        # Make trading decision if automated trading is enabled and token is active\n",
    "        if self.automated_trading_enabled and token in self.active_tokens:\n",
    "            await self.make_trading_decision(token, timeframe, candle)\n",
    "\n",
    "    def update_candle_history(self, token, timeframe, candle):\n",
    "        key = (token, timeframe)\n",
    "        if key not in self.candle_history:\n",
    "            self.candle_history[key] = deque(maxlen=10)\n",
    "        self.candle_history[key].append(candle['close'])\n",
    "\n",
    "    def get_candle_average(self, token, timeframe):\n",
    "        key = (token, timeframe)\n",
    "        if key in self.candle_history and len(self.candle_history[key]) > 0:\n",
    "            return sum(self.candle_history[key]) / len(self.candle_history[key])\n",
    "        return None\n",
    "\n",
    "    def get_log_file_path(self, token, timeframe):\n",
    "        return os.path.join(self.log_directory, f\"{token}_{timeframe}_trading_log.csv\")\n",
    "\n",
    "    def log_to_csv(self, token, timeframe, data):\n",
    "        file_path = self.get_log_file_path(token, timeframe)\n",
    "        file_exists = os.path.isfile(file_path)\n",
    "        \n",
    "        with open(file_path, mode='a', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=data.keys())\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(data)\n",
    "\n",
    "    async def make_trading_decision(self, token, timeframe, candle):\n",
    "        signal = \"Neutral\"\n",
    "        reasons = []\n",
    "\n",
    "        if self.active_indicators['supertrend'] and self.active_indicators['jma']:\n",
    "            if candle['supertrend_direction'] == 1 and candle['jma_direction'] == 1:\n",
    "                signal = \"Bullish\"\n",
    "                reasons.append(\"Supertrend and JMA are bullish\")\n",
    "            elif candle['supertrend_direction'] == -1 and candle['jma_direction'] == -1:\n",
    "                signal = \"Bearish\"\n",
    "                reasons.append(\"Supertrend and JMA are bearish\")\n",
    "\n",
    "        if self.active_indicators['candle_average']:\n",
    "            avg = self.get_candle_average(token, timeframe)\n",
    "            if avg is not None:\n",
    "                if candle['close'] > avg:\n",
    "                    reasons.append(\"Price is above 10-candle average\")\n",
    "                    if signal != \"Bearish\":\n",
    "                        signal = \"Bullish\"\n",
    "                elif candle['close'] < avg:\n",
    "                    reasons.append(\"Price is below 10-candle average\")\n",
    "                    if signal != \"Bullish\":\n",
    "                        signal = \"Bearish\"\n",
    "\n",
    "        self.logger.info(f\"Signal for {token} at {timeframe}: {signal}. Reasons: {', '.join(reasons)}\")\n",
    "\n",
    "        # Prepare data for logging\n",
    "        log_data = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'timeframe': timeframe,\n",
    "            'candle_time': candle.get('tt', ''),\n",
    "            'open': candle.get('open', ''),\n",
    "            'high': candle.get('high', ''),\n",
    "            'low': candle.get('low', ''),\n",
    "            'close': candle.get('close', ''),\n",
    "            'supertrend': candle.get('supertrend', ''),\n",
    "            'supertrend_direction': candle.get('supertrend_direction', ''),\n",
    "            'jma': candle.get('jma', ''),\n",
    "            'jma_direction': candle.get('jma_direction', ''),\n",
    "            'candle_average': self.get_candle_average(token, timeframe),\n",
    "            'signal': signal,\n",
    "            'reasons': ', '.join(reasons)\n",
    "        }\n",
    "        # Log to CSV\n",
    "        self.log_to_csv(token, timeframe, log_data)\n",
    "\n",
    "        # Implement your trading logic here based on the signal\n",
    "\n",
    "    # Methods to control trading settings\n",
    "    def enable_automated_trading(self):\n",
    "        self.automated_trading_enabled = True\n",
    "        self.logger.info(\"Automated trading enabled\")\n",
    "\n",
    "    def disable_automated_trading(self):\n",
    "        self.automated_trading_enabled = False\n",
    "        self.logger.info(\"Automated trading disabled\")\n",
    "\n",
    "    def add_active_token(self, token):\n",
    "        self.active_tokens.add(token)\n",
    "        self.logger.info(f\"Added {token} to active trading tokens\")\n",
    "\n",
    "    def remove_active_token(self, token):\n",
    "        self.active_tokens.discard(token)\n",
    "        self.logger.info(f\"Removed {token} from active trading tokens\")\n",
    "\n",
    "    def enable_indicator(self, indicator):\n",
    "        if indicator in self.active_indicators:\n",
    "            self.active_indicators[indicator] = True\n",
    "            self.logger.info(f\"Enabled {indicator} for trading decisions\")\n",
    "\n",
    "    def disable_indicator(self, indicator):\n",
    "        if indicator in self.active_indicators:\n",
    "            self.active_indicators[indicator] = False\n",
    "            self.logger.info(f\"Disabled {indicator} for trading decisions\")\n",
    "\n",
    "async def main():\n",
    "    global global_data_processor\n",
    "    global global_candle_end_finder, global_tick_collector, global_decision_maker\n",
    "    collector = TickCollector()\n",
    "    processor = DataProcessor(collector)\n",
    "    candle_finder = CandleEndFinder(processor)\n",
    "    trading_decision_maker = TradingDecisionMaker(candle_finder)\n",
    "    \n",
    "    global_tick_collector = collector\n",
    "    global_data_processor = processor\n",
    "    global_candle_end_finder = candle_finder\n",
    "    global_decision_maker = trading_decision_maker\n",
    "    \n",
    "    await asyncio.gather(collector.run(), processor.run(), candle_finder.run(),trading_decision_maker.run())\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.set_debug(True)\n",
    "if loop.is_running():\n",
    "    nest_asyncio.apply()\n",
    "asyncio.create_task(main())\n",
    "if not loop.is_running():\n",
    "    try:\n",
    "        loop.run_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Received exit signal. Cleaning up...\")\n",
    "    finally:\n",
    "        loop.close()\n",
    "        logger.info(\"Event loop closed. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_tick_collector.resampled_buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_completed_candles(token=None, timeframe=None):\n",
    "    global global_candle_end_finder    \n",
    "    if global_candle_end_finder is None:\n",
    "        raise ValueError(\"global_candle_end_finder is not initialized\")    \n",
    "    try:\n",
    "        completed_candles = await global_candle_end_finder.get_completed_candles(token, timeframe)\n",
    "        if completed_candles:\n",
    "            return completed_candles[-1]  # Return the last value\n",
    "        return None  # Return None if the list is empty\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting completed candles: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "async def example_usage():    \n",
    "    last_candle = await get_completed_candles(token=\"432294\", timeframe=\"5s\")\n",
    "    print(\"Last 1min candle for MCX|432294:\", last_candle)\n",
    "\n",
    "    last_candle = await get_completed_candles(token=\"432294\", timeframe=\"15s\")\n",
    "    print(\"Last 5min candle for MCX|432294:\", last_candle)\n",
    "\n",
    "    last_candle = await get_completed_candles(token=\"26000\", timeframe=\"5s\")\n",
    "    print(\"Last 1min candle for MCX|432294:\", last_candle)\n",
    "\n",
    "    last_candle = await get_completed_candles(token=\"26000\", timeframe=\"15s\")\n",
    "    print(\"Last 5min candle for MCX|432294:\", last_candle)\n",
    "\n",
    "# To run the example:\n",
    "asyncio.run(example_usage())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All contents:\n",
      "Token 432294:\n",
      "  Timeframe 5s: 28 entries\n",
      "    Latest entry: {'tt': Timestamp('2024-10-14 23:00:45'), 'open': 6247.0, 'high': 6247.0, 'low': 6247.0, 'close': 6247.0, 'supertrend': 6244.66, 'supertrend_direction': 1.0, 'jma': 6246.0, 'jma_direction': 1.0}\n",
      "  Timeframe 15s: 16 entries\n",
      "    Latest entry: {'tt': Timestamp('2024-10-14 23:00:45'), 'open': 6247.0, 'high': 6247.0, 'low': 6247.0, 'close': 6247.0, 'supertrend': 6243.24, 'supertrend_direction': 1.0, 'jma': 6244.98, 'jma_direction': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Function to get resampled buffer contents\n",
    "async def get_buffer_contents(token=None, timeframe=None):\n",
    "    global global_data_processor\n",
    "    if global_data_processor is None:\n",
    "        return \"DataProcessor not initialized yet. Please wait a moment and try again.\"\n",
    "    return await global_data_processor.get_resampled_buffer_contents(token, timeframe)\n",
    "\n",
    "# Get all resampled buffer contents\n",
    "all_contents = await get_buffer_contents()\n",
    "print(\"All contents:\")\n",
    "for token, timeframes in all_contents.items():\n",
    "    print(f\"Token {token}:\")\n",
    "    for tf, data in timeframes.items():\n",
    "        print(f\"  Timeframe {tf}: {len(data)} entries\")\n",
    "        if data:\n",
    "            print(f\"    Latest entry: {data[-1]}\")  # Show only the latest entry\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
