{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3df69203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 12:55:40,551 - ShoonyaApi - INFO - Starting candle_end_finder\n",
      "2024-10-07 12:55:45,004 - ShoonyaApi - WARNING - resampled_data is empty.\n",
      "2024-10-07 12:55:45,572 - ShoonyaApi - INFO - FeedJson for token 26009 is empty. LotSize: 15.\n",
      "2024-10-07 12:55:50,582 - ShoonyaApi - INFO - FeedJson for token 26009 is empty. LotSize: 15.\n",
      "2024-10-07 12:55:55,605 - ShoonyaApi - INFO - FeedJson for token 26009 is empty. LotSize: 15.\n",
      "2024-10-07 12:56:00,002 - ShoonyaApi - WARNING - resampled_data is empty.\n",
      "2024-10-07 12:56:00,631 - ShoonyaApi - INFO - FeedJson for token 26009 is empty. LotSize: 15.\n",
      "2024-10-07 12:56:05,328 - ShoonyaApi - INFO - Feed Opened\n",
      "2024-10-07 12:56:05,678 - ShoonyaApi - INFO - FeedJson for token 26009 is empty. LotSize: 15.\n",
      "2024-10-07 12:56:05,683 - ShoonyaApi - INFO - WebSocket connected and subscribed successfully.\n",
      "2024-10-07 12:56:15,007 - ShoonyaApi - INFO - Direction change detected for token 26009 at 2024-10-07 12:56:00: 0.0\n",
      "2024-10-07 12:57:45,012 - ShoonyaApi - INFO - Direction change detected for token 26009 at 2024-10-07 12:57:30: 1.0\n",
      "2024-10-07 13:01:00,004 - ShoonyaApi - INFO - Direction change detected for token 26009 at 2024-10-07 13:00:45: -1.0\n",
      "2024-10-07 13:04:30,003 - ShoonyaApi - INFO - Direction change detected for token 26009 at 2024-10-07 13:04:15: 1.0\n",
      "2024-10-07 13:07:15,004 - ShoonyaApi - INFO - Direction change detected for token 26009 at 2024-10-07 13:07:00: -1.0\n",
      "2024-10-07 13:07:15,007 - ShoonyaApi - WARNING - entering normal trade.\n",
      "2024-10-07 13:07:15,008 - ShoonyaApi - ERROR - Unable to get entry price for BANKNIFTY09OCT24P50800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44108 not found in extra_feedJson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 13:07:30,005 - ShoonyaApi - INFO - Direction change detected for token 26009 at 2024-10-07 13:07:15: 1.0\n",
      "2024-10-07 13:07:45,004 - ShoonyaApi - INFO - Direction change detected for token 26009 at 2024-10-07 13:07:30: -1.0\n",
      "2024-10-07 13:07:45,006 - ShoonyaApi - WARNING - entering normal trade.\n",
      "2024-10-07 13:07:45,012 - ShoonyaApi - ERROR - Unable to get entry price for BANKNIFTY09OCT24P50800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44108 not found in extra_feedJson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 13:11:00,023 - ShoonyaApi - INFO - Direction change detected for token 26009 at 2024-10-07 13:10:45: 1.0\n",
      "2024-10-07 13:12:30,003 - ShoonyaApi - INFO - Direction change detected for token 26009 at 2024-10-07 13:12:15: -1.0\n",
      "2024-10-07 13:12:30,005 - ShoonyaApi - WARNING - entering normal trade.\n",
      "2024-10-07 13:12:30,008 - ShoonyaApi - ERROR - Unable to get entry price for BANKNIFTY09OCT24P50700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44105 not found in extra_feedJson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 13:12:45,003 - ShoonyaApi - INFO - Direction change detected for token 26009 at 2024-10-07 13:12:30: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the CSV files into DataFrames\n",
    "nfo_scrips_df = pd.read_csv('/home/bitsbytes2algoz/Desktop/new/NFO_symbols.csv')\n",
    "mcx_scrips_df = pd.read_csv('/home/bitsbytes2algoz/Desktop/new/NFO_symbols.csv')\n",
    "\n",
    "async def get_lotsize():\n",
    "    global atm_strike\n",
    "     \n",
    "    while True:\n",
    "        await asyncio.sleep(5)\n",
    "        \n",
    "        if exchange.upper() == 'NSE':\n",
    "            df = nfo_scrips_df\n",
    "        elif exchange.upper() == 'MCX':\n",
    "            df = mcx_scrips_df\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported exchange. Please use 'NSE' or 'MCX'.\")\n",
    "\n",
    "        # Find the first matching symbol\n",
    "        match = df[df['Symbol'] == symbol]\n",
    "        \n",
    "        if not match.empty:\n",
    "            # Get LotSize\n",
    "            lotsize = match.iloc[0]['LotSize']\n",
    "            \n",
    "            # Check feedJson if token is provided\n",
    "            if Initial_token:\n",
    "                if not feedJson[Initial_token]:\n",
    "                    logger.info(f\"FeedJson for token {Initial_token} is empty. LotSize: {lotsize}.\")                    \n",
    "                else:\n",
    "                    # Retrieve the last recent price\n",
    "                    last_price = int(feedJson[Initial_token][-1]['ltp'])\n",
    "                    # Calculate ATM strike price\n",
    "                    mod = int(last_price) % 100\n",
    "                    atm_strike = last_price - mod if mod <= 50 else last_price + (100 - mod)                    \n",
    "                    # Find trading symbols for ATM strike\n",
    "                    filtered_df = df[\n",
    "                        (df['Symbol'] == symbol) &\n",
    "                        (df['StrikePrice'] == float(atm_strike))\n",
    "                    ]\n",
    "                    \n",
    "                    if filtered_df.empty:\n",
    "                        logger.info(f\"Could not find trading symbols for ATM strike {atm_strike}\")\n",
    "                        #return lotsize, last_price, None, None\n",
    "                        logger.info(f\"wait for lot size to update {lotsize}.\")\n",
    "\n",
    "                    # Find the CE and PE trading symbols\n",
    "                    ce_row = filtered_df[filtered_df['OptionType'] == 'CE'].sort_values('Expiry').iloc[0]\n",
    "                    pe_row = filtered_df[filtered_df['OptionType'] == 'PE'].sort_values('Expiry').iloc[0]\n",
    "\n",
    "                    ce_trading_symbol, pe_trading_symbol = ce_row['TradingSymbol'], pe_row['TradingSymbol']\n",
    "                    ce_trading_token, pe_trading_token = ce_row['Token'], pe_row['Token']                    \n",
    "                    \n",
    "                      # Update the state\n",
    "                    state.ce_trading_symbol = ce_trading_symbol\n",
    "                    state.pe_trading_symbol = pe_trading_symbol\n",
    "                    state.ce_trading_token = ce_trading_token\n",
    "                    state.pe_trading_token = pe_trading_token\n",
    "\n",
    "                    #logger.info(f\"CE Symbol: {ce_trading_symbol}, PE Symbol: {pe_trading_symbol}\")            \n",
    "        else:\n",
    "            logger.warning(f\"{symbol} not found on {exchange}.\")\n",
    "\n",
    "async def resample_ticks():\n",
    "    global resampled_data\n",
    "    \n",
    "    while True:\n",
    "        if not feedJson:\n",
    "            await asyncio.sleep(1)\n",
    "            continue\n",
    "\n",
    "        with feed_lock:\n",
    "            temp_resampled_data = {}\n",
    "            for token, ticks in feedJson.items():\n",
    "                try:\n",
    "                    if ticks:\n",
    "                        # Create a DataFrame with the new ticks\n",
    "                        df_new = pd.DataFrame(ticks)\n",
    "                        df_new[\"tt\"] = pd.to_datetime(df_new[\"tt\"])\n",
    "                        df_new.set_index(\"tt\", inplace=True)\n",
    "\n",
    "                        # Determine the current resample interval\n",
    "                        current_resample_interval = df_new.index.max().floor(resample_frequency)\n",
    "\n",
    "                        # Initialize or update the resampled DataFrame\n",
    "                        if token not in resampled_data:\n",
    "                            # Initialize the DataFrame with the first resample\n",
    "                            df_resampled = df_new['ltp'].resample(resample_frequency).ohlc()\n",
    "                            temp_resampled_data[token] = df_resampled\n",
    "                            last_resample_time[token] = df_resampled.index.max()\n",
    "                        else:\n",
    "                            # Create a copy of the existing data\n",
    "                            temp_df = resampled_data[token].copy()                            \n",
    "                            # Resample the new ticks\n",
    "                            df_resampled = df_new['ltp'].resample(resample_frequency).ohlc()\n",
    "\n",
    "                            # Update the temporary DataFrame with new data\n",
    "                            for idx, row in df_resampled.iterrows():\n",
    "                                if idx in temp_df.index:\n",
    "                                    temp_df.loc[idx, 'high'] = max(temp_df.loc[idx, 'high'], row['high'])\n",
    "                                    temp_df.loc[idx, 'low'] = min(temp_df.loc[idx, 'low'], row['low'])\n",
    "                                    temp_df.loc[idx, 'close'] = row['close']\n",
    "                                else:\n",
    "                                    temp_df.loc[idx] = row\n",
    "\n",
    "                            # Update the last resampled time\n",
    "                            last_resample_time[token] = df_resampled.index.max()                            \n",
    "                            temp_resampled_data[token] = temp_df\n",
    "\n",
    "                        if token in temp_resampled_data:\n",
    "                            supertrend, su_direction = numba_indicators.supertrend_numba(\n",
    "                                temp_resampled_data[token]['high'].values,\n",
    "                                temp_resampled_data[token]['low'].values,\n",
    "                                temp_resampled_data[token]['close'].values\n",
    "                                \n",
    "                            )\n",
    "                            temp_resampled_data[token]['supertrend'] = supertrend\n",
    "                            temp_resampled_data[token]['su_direction'] = su_direction\n",
    "\n",
    "                        if token in temp_resampled_data:\n",
    "                            jma, ce_direction = numba_indicators.jma_numba_direction(\n",
    "                                temp_resampled_data[token]['close'].values                        #       \n",
    "                            )                            \n",
    "                            temp_resampled_data[token]['jma'] = jma\n",
    "                            temp_resampled_data[token]['ce_direction'] = ce_direction                            \n",
    "\n",
    "                        # Clean up the data\n",
    "                        temp_resampled_data[token] = temp_resampled_data[token].dropna(subset=['open', 'high', 'low', 'close'])\n",
    "\n",
    "                except Exception as e:\n",
    "                    if isinstance(e, KeyError) and e.args[0] == 'tt':\n",
    "                        print(f\"Market likely closed for token {token}\")\n",
    "                    else:\n",
    "                        print(f\"Error resampling data for token {token}: {e}\")\n",
    "\n",
    "            # After processing all tokens, update the global resampled_data\n",
    "            resampled_data = temp_resampled_data\n",
    "            #print(resampled_data)\n",
    "\n",
    "        await asyncio.sleep(0.1)\n",
    "\n",
    "from collections import defaultdict\n",
    "completed_candles_dfs = defaultdict(pd.DataFrame)\n",
    "last_processed_candle = defaultdict(pd.Timestamp)\n",
    "\n",
    "async def candle_end_finder():\n",
    "    global resampled_data, resample_frequency\n",
    "    logger.info(\"Starting candle_end_finder\")\n",
    "\n",
    "    while True:\n",
    "        current_time = pd.Timestamp.now()\n",
    "        time_bucket_start = current_time.floor(resample_frequency)\n",
    "        time_bucket_end = time_bucket_start + pd.Timedelta(resample_frequency)\n",
    "        wait_time = (time_bucket_end - current_time).total_seconds() + 0.001\n",
    "\n",
    "        if wait_time > 0:\n",
    "            await asyncio.sleep(wait_time)\n",
    "\n",
    "        data_copy = {}\n",
    "        with feed_lock:\n",
    "            if not resampled_data:\n",
    "                logger.warning(\"resampled_data is empty.\")\n",
    "                continue\n",
    "\n",
    "            # Copying the data outside of the lock to avoid data modification during processing\n",
    "            for token, df in resampled_data.items():\n",
    "                if df is None or df.empty:\n",
    "                    logger.warning(f\"DataFrame for token {token} is empty or None.\")\n",
    "                    continue\n",
    "\n",
    "                # if len(df) < 3:\n",
    "                #     logger.warning(f\"Not enough data for token {token}. Need at least 3 candles.\")\n",
    "                #     continue\n",
    "\n",
    "                data_copy[token] = df.copy()\n",
    "\n",
    "        # Process each token's data\n",
    "        for token, df in data_copy.items():\n",
    "            logger.debug(f\"Processing token: {token}\")\n",
    "\n",
    "            # Checking if this is the first time processing or if the bucket has moved forward\n",
    "            if token not in last_processed_candle or last_processed_candle[token] < time_bucket_end:\n",
    "                try:\n",
    "                    previous_length = len(completed_candles_dfs[token]) if token in completed_candles_dfs else 0\n",
    "\n",
    "                    if time_bucket_start == df.index[-1]:\n",
    "                        # Only include the last candle if it matches the bucket start\n",
    "                        completed_candles_dfs[token] = df\n",
    "                        last_processed_candle[token] = time_bucket_start\n",
    "\n",
    "                    elif time_bucket_end == df.index[-1]:\n",
    "                        # If the end bucket matches, take all up to that point\n",
    "                        completed_candles_dfs[token] = df.loc[:time_bucket_end - pd.Timedelta(microseconds=1)]\n",
    "                        last_processed_candle[token] = time_bucket_start\n",
    "                    else:\n",
    "                        logger.error(f\"No new candle forming now for token {token}\")\n",
    "                        continue\n",
    "                                        \n",
    "                    new_length = len(completed_candles_dfs[token])\n",
    "                    rows_added = new_length - previous_length\n",
    "                    \n",
    "                    if rows_added > 1:\n",
    "                        logger.warning(f\"Multiple rows ({rows_added}) added for token {token} at {pd.Timestamp.now()}. \"\n",
    "                        f\"Previous length: {previous_length}, New length: {new_length}\")\n",
    "                    # Verify that the required columns exist\n",
    "                    try:\n",
    "                        required_columns = ['ce_direction']\n",
    "                        missing_columns = [col for col in required_columns if col not in completed_candles_dfs[token].columns]\n",
    "                        if missing_columns:\n",
    "                            raise KeyError(f\"Missing columns {missing_columns}\")\n",
    "\n",
    "                    except KeyError as e:\n",
    "                        logger.error(f\"Error processing token {token}: {e}\")\n",
    "\n",
    "                except KeyError as e:\n",
    "                    logger.error(f\"Error processing token {token}: Column {e} not found.\")\n",
    "                except IndexError:\n",
    "                    logger.error(f\"Error processing token {token}: Not enough candles to calculate indicators.\")\n",
    "\n",
    "        # Calculate the time for the next bucket\n",
    "        next_bucket_start = time_bucket_end\n",
    "        wait_time = (next_bucket_start - pd.Timestamp.now()).total_seconds()\n",
    "        if wait_time > 0:\n",
    "            await asyncio.sleep(wait_time)\n",
    "\n",
    "def get_latest_price_option(entry_instrument):\n",
    "    entry_instrument_str = str(entry_instrument)\n",
    "    with feed_lock:\n",
    "        if entry_instrument_str in extra_feedJson:\n",
    "            latest_data = extra_feedJson[entry_instrument_str][-1]\n",
    "            latest_price = latest_data['ltp']\n",
    "            \n",
    "            return latest_price\n",
    "        else:\n",
    "            print(f\"{entry_instrument_str} not found in extra_feedJson\")  # Debug print\n",
    "            return None\n",
    "        \n",
    "def get_latest_price_index(entry_instrument):\n",
    "    entry_instrument_str = str(entry_instrument)\n",
    "    with feed_lock:\n",
    "        if entry_instrument_str in feedJson:\n",
    "            latest_data = feedJson[entry_instrument_str][-1]\n",
    "            latest_price = latest_data['ltp']\n",
    "            \n",
    "            return latest_price\n",
    "        else:\n",
    "            print(f\"{entry_instrument_str} not found in feedJson\")  # Debug print\n",
    "            return None\n",
    "        \n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pyotp \n",
    "from asyncio import CancelledError\n",
    "from collections import defaultdict, deque\n",
    "from threading import Thread\n",
    "from threading import Lock\n",
    "from datetime import datetime\n",
    "import numba_indicators\n",
    "import uuid\n",
    "from NorenRestApiPy.NorenApi import NorenApi\n",
    "\n",
    "# Initialize logger\n",
    "logger = logging.getLogger('ShoonyaApi')\n",
    "logger.setLevel(logging.INFO) \n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "file_handler = logging.FileHandler('shoonya_api.log') # Replace 'shoonya_api.log' with your desired log file name\n",
    "file_handler.setLevel(logging.INFO)  # Set the logging level for the file handler\n",
    "# Apply the same formatter to the file handler\n",
    "file_handler.setFormatter(formatter)\n",
    "# Add the file handler to the logger\n",
    "logger.addHandler(file_handler) \n",
    "\n",
    "# Global variables\n",
    "resampled_data = {}\n",
    "last_resample_time = {}\n",
    "monitoring_tasks = {}\n",
    "last_processed_candle = {} \n",
    "current_positions = {}\n",
    "feed_opened = False\n",
    "feed_lock = Lock()\n",
    "position_lock = asyncio.Lock()\n",
    "feedJson = defaultdict(lambda: deque(maxlen=20))\n",
    "extra_feedJson = defaultdict(lambda: deque(maxlen=20))\n",
    "api = None\n",
    "exchange = 'NSE'\n",
    "Initial_token = '26009'\n",
    "subscription_string = f\"{exchange}|{Initial_token}\"\n",
    "resample_frequency = \"15s\"\n",
    "symbol = \"BANKNIFTY\"\n",
    "atm_strike = int\n",
    "\n",
    "# TRADE CONTROL flags \n",
    "enable_call_trades = True\n",
    "enable_put_trades = True\n",
    "enable_all_trades = True\n",
    "force_exit_triggered = False\n",
    "\n",
    "class TradingState:\n",
    "    def __init__(self):\n",
    "        self.ce_trading_symbol = None\n",
    "        self.pe_trading_symbol = None\n",
    "        self.ce_trading_token = None\n",
    "        self.pe_trading_token = None\n",
    "state = TradingState()\n",
    "\n",
    "\n",
    "def initialize_api(credentials_file=\"usercred.xlsx\"):\n",
    "    global api\n",
    "    api = NorenApi(\n",
    "        host=\"https://api.shoonya.com/NorenWClientTP/\",\n",
    "        websocket=\"wss://api.shoonya.com/NorenWSTP/\"\n",
    "    )\n",
    "\n",
    "    credentials = pd.read_excel(credentials_file)\n",
    "    user = credentials.iloc[0, 0]\n",
    "    password = credentials.iloc[0, 1]\n",
    "    vendor_code = credentials.iloc[0, 2]\n",
    "    app_key = credentials.iloc[0, 3]\n",
    "    imei = credentials.iloc[0, 4]\n",
    "    qr_code = credentials.iloc[0, 5]\n",
    "    factor2 = pyotp.TOTP(qr_code).now()\n",
    "\n",
    "    api.login_result = api.login(\n",
    "        userid=user,\n",
    "        password=password,\n",
    "        twoFA=factor2,\n",
    "        vendor_code=vendor_code,\n",
    "        api_secret=app_key,\n",
    "        imei=imei\n",
    "    )\n",
    "\n",
    "def event_handler_order_update(data):\n",
    "    logger.info(f\"Order update: {data}\")\n",
    "\n",
    "def event_handler_feed_update(tick_data):\n",
    "    #print(tick_data)\n",
    "    try:\n",
    "        if 'lp' in tick_data and 'tk' in tick_data:\n",
    "            timest = datetime.fromtimestamp(int(tick_data['ft'])).isoformat()\n",
    "            token = tick_data['tk']\n",
    "\n",
    "            with feed_lock:  # Acquire lock for thread-safety\n",
    "\n",
    "                    if token == Initial_token:\n",
    "                        feedJson[token].append({'ltp': float(tick_data['lp']), 'tt': timest})\n",
    "                    else:\n",
    "                        extra_feedJson[token].append({'ltp': float(tick_data['lp']), 'tt': timest})\n",
    "\n",
    "    except (KeyError, ValueError) as e:\n",
    "        logger.error(f\"Error processing tick data: {e}\")\n",
    "\n",
    "\n",
    "async def connect_and_subscribe():\n",
    "    global feed_opened\n",
    "    retry_delay = 1  # Initial retry delay in seconds\n",
    "    max_retry_delay = 32  # Maximum retry delay in seconds\n",
    "    while True:\n",
    "        try:\n",
    "            api.start_websocket(\n",
    "                order_update_callback=event_handler_order_update,\n",
    "                subscribe_callback=event_handler_feed_update,\n",
    "                socket_open_callback=open_callback,\n",
    "                socket_close_callback=close_callback\n",
    "            )\n",
    "            await wait_for_feed_open(timeout=30)  # Wait for feed to open with a timeout\n",
    "            api.subscribe([subscription_string])\n",
    "            logger.info(\"WebSocket connected and subscribed successfully.\")\n",
    "            retry_delay = 1  # Reset retry delay after successful connection\n",
    "            await monitor_connection()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"WebSocket connection error: {e}\")\n",
    "            logger.info(f\"Reconnecting in {retry_delay} seconds...\")\n",
    "            await asyncio.sleep(retry_delay)\n",
    "            retry_delay = min(retry_delay * 2, max_retry_delay)  # Exponential backoff\n",
    "\n",
    "async def wait_for_feed_open(timeout):\n",
    "    global feed_opened\n",
    "    start_time = asyncio.get_event_loop().time()\n",
    "    while not feed_opened:\n",
    "        if asyncio.get_event_loop().time() - start_time > timeout:\n",
    "            raise TimeoutError(\"Timed out waiting for feed to open\")\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "async def monitor_connection():\n",
    "    global feed_opened\n",
    "    while True:\n",
    "        if not feed_opened:\n",
    "            logger.warning(\"Feed closed unexpectedly. Reconnecting...\")\n",
    "            raise Exception(\"Feed closed\")\n",
    "        await asyncio.sleep(5)  # Check connection status every 5 seconds\n",
    "\n",
    "def close_callback():\n",
    "    global feed_opened\n",
    "    feed_opened = False\n",
    "    logger.warning(\"WebSocket connection closed.\")\n",
    "\n",
    "def open_callback():\n",
    "    global feed_opened\n",
    "    if not feed_opened:\n",
    "        feed_opened = True\n",
    "        logger.info('Feed Opened')\n",
    "    else:\n",
    "        logger.warning('Feed Opened callback called multiple times.')\n",
    "\n",
    "async def main():\n",
    "    initialize_api()\n",
    "    websocket_task = asyncio.create_task(connect_and_subscribe())\n",
    "    resample_task = asyncio.create_task(resample_ticks())\n",
    "    lot_size_task = asyncio.create_task(get_lotsize()) \n",
    "    candle_end_finder_task = asyncio.create_task(candle_end_finder())     \n",
    "    direction_change_event_handler_task = asyncio.create_task(direction_change_event_handler())\n",
    "    process_direction_changes_task = asyncio.create_task(process_direction_changes())\n",
    "    await asyncio.gather(websocket_task, lot_size_task, resample_task, candle_end_finder_task, direction_change_event_handler_task, process_direction_changes_task)\n",
    "# Your existing event loop setup\n",
    "loop = asyncio.get_event_loop()\n",
    "if loop.is_running():\n",
    "    nest_asyncio.apply()\n",
    "asyncio.create_task(main())\n",
    "if not loop.is_running():\n",
    "    loop.run_forever()\n",
    "\n",
    "\n",
    "direction_change_queue = asyncio.Queue()\n",
    "async def direction_change_event_handler():\n",
    "    last_changes = {}\n",
    "    while True:\n",
    "        for token, df in completed_candles_dfs.items():\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            current_direction = df['ce_direction'].iloc[-1]\n",
    "            second_direction = df['su_direction'].iloc[-1]   ########################################################################################\n",
    "            last_direction = last_changes.get(token, {}).get('ce_direction', None)\n",
    "\n",
    "            # Check if the 'ce_direction' value has changed\n",
    "            if last_direction is None or current_direction != last_direction:\n",
    "                logger.info(f\"Direction change detected for token {token} at {df.index[-1]}: {current_direction}\")\n",
    "                await direction_change_queue.put((token, current_direction, last_direction, second_direction)) ########################################################################################\n",
    "\n",
    "            # Update last changes\n",
    "            last_changes[token] = {'index': df.index[-1], 'ce_direction': current_direction}\n",
    "\n",
    "        await asyncio.sleep(0.001)\n",
    "\n",
    "\n",
    "async def process_direction_changes():\n",
    "    while True:\n",
    "        token, current_direction, previous_direction, second_direction = await direction_change_queue.get() ########################################################################################\n",
    "        try:\n",
    "            await execute_trade_logic(token, current_direction, previous_direction, second_direction) ########################################################################################\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error executing trade logic for {token}: {e}\")\n",
    "\n",
    "        direction_change_queue.task_done()\n",
    "\n",
    "############################################################     \n",
    "class TradeTracker:\n",
    "    def __init__(self):\n",
    "        self.total_trades = 0\n",
    "        self.total_points_collected = 0\n",
    "        self.total_points_lost = 0\n",
    "        self.overall_profit_loss = 0\n",
    "        self.win_trades = 0\n",
    "        self.loss_trades = 0\n",
    "        self.zero_pnl_trades = 0  # New attribute to track zero P/L trades\n",
    "\n",
    "    def update_stats(self, profit_loss):\n",
    "        self.total_trades += 1\n",
    "\n",
    "        if profit_loss > 0:\n",
    "            self.win_trades += 1\n",
    "            self.total_points_collected += profit_loss\n",
    "        elif profit_loss < 0:\n",
    "            self.loss_trades += 1\n",
    "            self.total_points_lost += abs(profit_loss)\n",
    "        else:  # Handle zero P/L trades\n",
    "            self.zero_pnl_trades += 1\n",
    "\n",
    "        self.overall_profit_loss += profit_loss\n",
    "\n",
    "    def print_summary(self):\n",
    "        logger.info(\"Trade Summary:\")\n",
    "        logger.info(f\"  Total Trades: {self.total_trades}\")\n",
    "        logger.info(f\"  Win Trades: {self.win_trades}\")\n",
    "        logger.info(f\"  Loss Trades: {self.loss_trades}\")\n",
    "        logger.info(f\"  Zero P/L Trades: {self.zero_pnl_trades}\")  # Add this line\n",
    "        logger.info(f\"  Total Points Collected: {self.total_points_collected:.2f}\")\n",
    "        logger.info(f\"  Total Points Lost: {self.total_points_lost:.2f}\")\n",
    "        logger.info(f\"  Overall Profit/Loss: {self.overall_profit_loss:.2f} points\")\n",
    "\n",
    "# Create a TradeTracker instance\n",
    "trade_tracker = TradeTracker()\n",
    "\n",
    "class Trade:\n",
    "    def __init__(self, token, position_type, entry_price, option_token, symbol_name):\n",
    "        self.token = token\n",
    "        self.position_type = position_type\n",
    "        self.entry_price = entry_price\n",
    "        self.option_token = option_token\n",
    "        self.symbol_name = symbol_name\n",
    "        self.monitoring_task = None\n",
    "        self.exit_event = asyncio.Event()\n",
    "        self.exited = False  # Flag to indicate if the trade has exited\n",
    "\n",
    "        # Initialize your strategy parameters here (target, stop loss, etc.)\n",
    "        self.highest_price = entry_price\n",
    "        self.target_price = entry_price + 20\n",
    "        self.sl_price = entry_price - 7\n",
    "        self.trailing_sl_price = self.sl_price\n",
    "        self.trail_activated = False\n",
    "        self.trail_activation_point = 4\n",
    "\n",
    "    def update_sl(self, new_sl_price):\n",
    "        \"\"\"Update the stop loss price.\"\"\"\n",
    "        if not self.exited:\n",
    "            self.sl_price = new_sl_price\n",
    "            logger.info(f\"SL updated for {self.symbol_name} to {self.sl_price}\")\n",
    "\n",
    "    def update_trailing_sl(self, new_trailing_sl_price):\n",
    "        \"\"\"Update the trailing stop loss price.\"\"\"\n",
    "        if not self.exited:\n",
    "            self.trailing_sl_price = new_trailing_sl_price\n",
    "            self.trail_activated = True ######################\n",
    "            logger.info(f\"Trailing SL updated for {self.symbol_name} to {self.trailing_sl_price}\")\n",
    "\n",
    "    async def force_exit(self, exit_reason=\"Forced Exit\"):\n",
    "        \"\"\"Force exit the trade immediately.\"\"\"\n",
    "        if not self.exited:\n",
    "            await exit_trade(self, exit_reason)\n",
    "            logger.info(f\"Trade forcibly exited for {self.symbol_name}\")\n",
    "        \n",
    "\n",
    "async def monitor_position(trade):\n",
    "    try:\n",
    "        while not trade.exited:\n",
    "            # Fetch real-time price \n",
    "            current_price = get_latest_price_option(trade.option_token)\n",
    "\n",
    "            if current_price is None:\n",
    "                await asyncio.sleep(0.05)\n",
    "                continue\n",
    "\n",
    "            # Check for target/stop loss/trailing stop loss (using trade's parameters)\n",
    "            if current_price >= trade.target_price:\n",
    "                await exit_trade(trade, \"Target Reached\")\n",
    "                break  # Exit the loop after exiting the trade\n",
    "            elif current_price <= trade.sl_price:\n",
    "                await exit_trade(trade, \"Stop Loss Hit\")\n",
    "                break  # Exit the loop after exiting the trade\n",
    "            elif not trade.trail_activated and current_price >= (trade.entry_price + trade.trail_activation_point):\n",
    "                trade.trail_activated = True\n",
    "                trade.trailing_sl_price = trade.entry_price\n",
    "                logger.info(f\"Trailing SL activated for {trade.symbol_name} at {trade.trailing_sl_price}\")\n",
    "            elif trade.trail_activated and current_price <= trade.trailing_sl_price:\n",
    "                await exit_trade(trade, \"Trailing Stop Loss Hit\")\n",
    "                break\n",
    "\n",
    "            # # Update highest_price for trailing stop loss\n",
    "            if current_price > trade.highest_price:\n",
    "                trade.highest_price = current_price\n",
    "            #     if trade.trail_activated:\n",
    "            #         trade.trailing_sl_price = trade.highest_price - (trade.trail_activation_point - 3)\n",
    "##########################################################################################################################################################\n",
    "            await asyncio.sleep(0.05)\n",
    "    except asyncio.CancelledError:\n",
    "        logger.info(f\"Monitoring cancelled for {trade.token} due to normal exit\")\n",
    "\n",
    "\n",
    "async def enter_trade(token, position_type, entry_type):\n",
    "    try:\n",
    "        option_token = state.ce_trading_token if position_type == \"call_buy\" else state.pe_trading_token\n",
    "        symbol_name = state.ce_trading_symbol if position_type == \"call_buy\" else state.pe_trading_symbol\n",
    "\n",
    "        entry_price = get_latest_price_option(option_token)  # Assuming this function exists\n",
    "        if entry_price is None:\n",
    "            logger.error(f\"Unable to get entry price for {symbol_name}\")\n",
    "            return\n",
    "\n",
    "        # Create a new Trade object\n",
    "        trade = Trade(token, position_type, entry_price, option_token, symbol_name)\n",
    "\n",
    "        # Update current_positions\n",
    "        current_positions[token] = trade\n",
    "\n",
    "        action_time = pd.Timestamp.now()\n",
    "        logger.warning(f\"Trade entered at {action_time} - {symbol_name} at {entry_price}\")\n",
    "\n",
    "        # Create a unique identifier for the trade and its monitoring task\n",
    "        trade_id = uuid.uuid4()\n",
    "\n",
    "        # Start the new monitoring task immediately\n",
    "        trade.monitoring_task = asyncio.create_task(monitor_position(trade))\n",
    "        monitoring_tasks[trade_id] = trade.monitoring_task\n",
    "\n",
    "        # Place actual order using your brokerage API (replace with your actual order placement logic)\n",
    "        # order_result = place_order(symbol_name, position_type, lot_size, entry_price)  # Example\n",
    "        # logger.info(f\"Order placed: {order_result}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in enter_trade for {token}: {e}\")\n",
    "        raise\n",
    "\n",
    "async def exit_trade(trade, exit_type):    \n",
    "    try:\n",
    "        exit_price = get_latest_price_option(trade.option_token)  # Assuming this function exists\n",
    "        if exit_price is None:\n",
    "            logger.error(f\"Unable to get exit price for {trade.symbol_name}\")\n",
    "            return\n",
    "\n",
    "        #profit_loss = (exit_price - trade.entry_price) / trade.entry_price * 100\n",
    "        profit_loss = (exit_price - trade.entry_price) \n",
    "\n",
    "        action_time = pd.Timestamp.now()\n",
    "        logger.warning(f\"Trade exited at {action_time} - {trade.symbol_name} at {exit_price}. {exit_type}. P/L: {profit_loss:.2f} points\")\n",
    "\n",
    "        # Mark the trade as exited and cancel the monitoring task\n",
    "        trade.exited = True\n",
    "        if trade.monitoring_task:\n",
    "            trade_id = next((tid for tid, task in monitoring_tasks.items() if task == trade.monitoring_task), None)\n",
    "            if trade_id:\n",
    "                monitoring_tasks[trade_id].cancel()\n",
    "                del monitoring_tasks[trade_id]\n",
    "        \n",
    "        trade_tracker.update_stats(profit_loss)\n",
    "\n",
    "        # Place actual exit order using your brokerage API (replace with your actual order placement logic)\n",
    "        # order_result = place_exit_order(trade.symbol_name, trade.position_type, lot_size, exit_price)  # Example\n",
    "        # logger.info(f\"Exit order placed: {order_result}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in exit_trade for {trade.token}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "async def execute_trade_logic(token, current_direction, previous_direction, second_direction): #########################################\n",
    "    global current_positions, position_lock\n",
    "\n",
    "    async with position_lock:\n",
    "        # Check for active trades for the token\n",
    "        active_trade = next((trade for trade in current_positions.values() if not trade.exited), None)\n",
    "\n",
    "        # Check for exit signals first, even if there's an active trade\n",
    "        if active_trade:\n",
    "            exit_signal = (\n",
    "                (active_trade.position_type == 'call_buy' and current_direction == -1 and previous_direction == 1) or\n",
    "                (active_trade.position_type == 'put_buy' and current_direction == 1 and previous_direction == -1)\n",
    "            )\n",
    "\n",
    "            if exit_signal:\n",
    "                try:\n",
    "                    logger.warning(\"exiting\")\n",
    "                    await exit_trade(active_trade, \"Regular Exit\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error exiting trade for {token}: {e}\")\n",
    "\n",
    "                # After exiting, set active_trade to None to allow for immediate new entry\n",
    "                active_trade = None\n",
    "\n",
    "       \n",
    "        if not active_trade:\n",
    "            entry_signal = False\n",
    "            if current_direction == 1 and previous_direction == -1 and enable_call_trades and enable_all_trades and second_direction == current_direction: #########################################\n",
    "                position_type = \"call_buy\"\n",
    "                entry_signal = True\n",
    "            elif current_direction == -1 and previous_direction == 1 and enable_put_trades and enable_all_trades and second_direction == current_direction: #########################################\n",
    "                position_type = \"put_buy\"\n",
    "                entry_signal = True\n",
    "\n",
    "            if entry_signal:\n",
    "                try:\n",
    "                    logger.warning(\"entering normal trade.\")\n",
    "                    await enter_trade(token, position_type, \"Regular Entry\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error entering trade for {token}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72852765",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade1 = current_positions.get('26037')  # Fetch the trade object\n",
    "\n",
    "if trade1:\n",
    "    trade1.update_sl(new_sl_price=trade1.entry_price - 2)  # Update SL dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade1 = current_positions.get('26009')  # Fetch the trade object\n",
    "if trade1:\n",
    "    \n",
    "    trade1.update_trailing_sl(new_trailing_sl_price=trade1.highest_price - 1)  # Update Trailing SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebcf70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade1 = current_positions.get('26009')  # Fetch the trade object\n",
    "if trade1:\n",
    "   \n",
    "    await trade1.force_exit()  # Force exit the trade if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_tracker.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769152ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_current_positions_info():\n",
    "    async with position_lock:  # Acquire the lock to ensure data consistency\n",
    "        for token, trade in current_positions.items():\n",
    "            print(f\"Token: {token}\")\n",
    "            print(f\"  Position Type: {trade.position_type}\")\n",
    "            print(f\"  Entry Price: {trade.entry_price}\")\n",
    "            print(f\"  Stop Loss Price: {trade.sl_price}\")\n",
    "            print(f\"  Trailing Stop Loss Price: {trade.trailing_sl_price}\")\n",
    "            print(f\"  Trailing SL Activated: {trade.trail_activated}\")\n",
    "            print(f\"  Trade Exited: {trade.exited}\")  # Add this line\n",
    "            print(\"------------------\")\n",
    "await get_current_positions_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5061c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_subscription():\n",
    "    op_chain = api.get_option_chain(exchange='NFO', tradingsymbol=state.ce_trading_symbol, strikeprice=atm_strike, count = 15)\n",
    "    tokens = [item['token'] for item in op_chain['values']]\n",
    "    subscriptions = [f\"NFO|{token}\" for token in tokens]\n",
    "    # Subscribe to each token\n",
    "    for sub in subscriptions:\n",
    "        api.subscribe(sub)\n",
    "\n",
    "    # Print the subscriptions for verification\n",
    "    print(\"Subscribed to:\", subscriptions)\n",
    "\n",
    "option_subscription()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a66149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def append_to_csv(trade_log):\n",
    "    with open('trading_log.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming resampled_data has the format {'token_name': DataFrame}\n",
    "token, df = next(iter(completed_candles_dfs.items()))  # Get the token name and DataFrame\n",
    "\n",
    "df.to_csv(f\"{token}.csv\", index=True)   # Save the DataFrame to a CSV file (include the index)\n",
    "print(f\"Saved data to {token}.csv\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
